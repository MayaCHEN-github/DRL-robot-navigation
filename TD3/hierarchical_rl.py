# å¯¼å…¥å¿…è¦çš„åº“
import os  # ç”¨äºæ–‡ä»¶å’Œç›®å½•æ“ä½œ
import time  # ç”¨äºæ—¶é—´æˆ³
import numpy as np  
import torch  
import optuna  # ç”¨äºè¶…å‚æ•°ä¼˜åŒ–
from stable_baselines3 import DQN, TD3  
from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback  # ä»sb3å¯¼å…¥çš„å›è°ƒ

# è‡ªå®šä¹‰æ—¥å¿—å›è°ƒç±»
class LoggingCallback(BaseCallback):
    def __init__(self, eval_freq: int, verbose: int = 0):
        super().__init__(verbose)
        self.eval_freq = eval_freq
        self.episode_rewards = []

    def _on_step(self) -> bool:
        if self.n_calls % self.eval_freq == 0:
            if self.verbose > 0:
                print(f"æ­¥æ•°: {self.n_calls}, å®Œæˆè¯„ä¼°å¹¶è®°å½•æ—¥å¿—")
        return True
# ä»sb3å¯¼å…¥çš„ç»éªŒå›æ”¾ç¼“å†²åŒº
from stable_baselines3.common.buffers import ReplayBuffer
from tqdm import tqdm  # ç”¨äºæ˜¾ç¤ºè¿›åº¦æ¡

# ç§»é™¤äº†PrioritizedReplayBufferçš„å¯¼å…¥ï¼Œå› ä¸ºè¯¥ç±»ä¸å¯ç”¨
PRIORITIZED_REPLAY_AVAILABLE = False
from gym_wrapper import VelodyneGymWrapper  # è‡ªå®šä¹‰çš„Gymç¯å¢ƒåŒ…è£…å™¨
from velodyne_env import GazeboEnv  # è‡ªå®šä¹‰çš„Gazeboç¯å¢ƒ
from typing import Dict, Any, Tuple, List, Optional  # ç±»å‹æ³¨è§£

# å¯¼å…¥gymæˆ–gymnasiumåº“(ä¿®ä¸æ˜ç™½ï¼)
try:
    import gymnasium
    from gymnasium.spaces import Box, Discrete
    gym_lib = 'gymnasium'
except ImportError:
    import gym
    from gym.spaces import Box, Discrete
    gym_lib = 'gym'

import sys, signal, atexit  # ç”¨äºä¿¡å·å¤„ç†ä¸è¿›ç¨‹é€€å‡ºæ¸…ç†

# Optunaé…ç½®
OPTUNA_STUDY_NAME = "hierarchical_rl_study"  # è¶…å‚æ•°ä¼˜åŒ–ç ”ç©¶çš„åç§°
OPTUNA_N_TRIALS = 50  # è¶…å‚æ•°æœç´¢çš„è¯•éªŒæ¬¡æ•°ï¼Œè®¾ç½®ä¸º50æ¬¡ä»¥å¹³è¡¡æœç´¢æ•ˆç‡å’Œæ•ˆæœ


class HierarchicalRL:
    """å±‚çº§å¼ºåŒ–å­¦ä¹ (HRL)æ¶æ„

    è¯¥ç±»å®ç°äº†ä¸€ä¸ªä¸¤å±‚çš„å¼ºåŒ–å­¦ä¹ æ¶æ„ï¼Œç”¨äºæœºå™¨äººå¯¼èˆªä»»åŠ¡ï¼š
    - é«˜å±‚(High-level): ä½¿ç”¨DQNå†³å®šå¯¼èˆªæ–¹å‘å’Œè·ç¦»
    - ä½å±‚(Low-level): ä½¿ç”¨TD3æ‰§è¡Œå…·ä½“çš„æ§åˆ¶åŠ¨ä½œ

    è¿™ç§åˆ†å±‚æ¶æ„çš„ä¼˜åŠ¿åœ¨äºå¯ä»¥å°†å¤æ‚çš„å¯¼èˆªä»»åŠ¡åˆ†è§£ä¸ºæ›´ç®€å•çš„å­ä»»åŠ¡ï¼Œæé«˜å­¦ä¹ æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚

    """
    def __init__(self, environment_dim=20, max_timesteps=2e6, eval_freq=5000, device=None, batch_train_size=100,
                 epsilon_start=1.0, epsilon_end=0.05, epsilon_decay=1e5,
                 noise_start=0.2, noise_end=0.01, noise_decay=1e5):
        # ç§»é™¤äº†ä¸PrioritizedReplayBufferç›¸å…³çš„å‚æ•°ï¼Œå› ä¸ºè¯¥ç±»ä¸å¯ç”¨
        use_per = False
        """åˆå§‹åŒ–HRL Agent"""
        # CUDAä½¿ç”¨è®¾ç½®
        self.device = torch.device("cpu")  # å¼ºåˆ¶ä½¿ç”¨CPUï¼Œä¸ä½¿ç”¨GPU
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

        # ç¯å¢ƒå‚æ•°
        self.environment_dim = environment_dim  # æ–¹å‘æ•°é‡ã€‚å°†æ–¹å‘åˆ‡åˆ†æˆé»˜è®¤20ä¸ªenvironment_dimï¼Œæ¯ä¸ªenvironment_dimè¡¨ç¤ºä¸€ä¸ªæ–¹å‘
        self.max_timesteps = max_timesteps  # æœ€å¤§è®­ç»ƒæ—¶é—´æ­¥ã€‚é»˜è®¤5e6
        self.eval_freq = eval_freq  # è¯„ä¼°é¢‘ç‡ï¼Œé»˜è®¤5e3
        self.max_ep = 500  # æ¯ä¸ªå›åˆçš„æœ€å¤§æ­¥æ•°ï¼Œé»˜è®¤500
        self.batch_train_size = batch_train_size  # æ‰¹é‡è®­ç»ƒå¤§å°ï¼Œé»˜è®¤100

        # ç»éªŒå›æ”¾å‚æ•°
        self.use_per = False  # å·²ç¦ç”¨ä¼˜å…ˆç»éªŒå›æ”¾(PER)ï¼Œå› ä¸ºPrioritizedReplayBufferä¸å¯ç”¨

        # æ¢ç´¢ç­–ç•¥å‚æ•°
        self.epsilon_start = epsilon_start  # DQNåˆå§‹æ¢ç´¢ç‡ï¼Œé»˜è®¤1.0
        self.epsilon_end = epsilon_end  # DQNæœ€ç»ˆæ¢ç´¢ç‡ï¼Œé»˜è®¤0.05
        self.epsilon_decay = epsilon_decay  # DQNæ¢ç´¢ç‡è¡°å‡é€Ÿç‡ï¼Œé»˜è®¤1e5
        self.noise_start = noise_start  # TD3åˆå§‹å™ªå£°æ°´å¹³ï¼Œé»˜è®¤0.2
        self.noise_end = noise_end  # TD3æœ€ç»ˆå™ªå£°æ°´å¹³ï¼Œé»˜è®¤0.01
        self.noise_decay = noise_decay  # TD3å™ªå£°è¡°å‡é€Ÿç‡
        self.current_epsilon = epsilon_start  # å½“å‰epsilonå€¼
        self.current_noise = noise_start  # å½“å‰å™ªå£°å€¼

        # è®­ç»ƒå¼€å§‹å‰æ”¶é›†çš„ç»éªŒæ•°é‡
        self.learn_starts = 1000  # é»˜è®¤ä¸º1000æ­¥

        # é«˜å±‚æ™ºèƒ½ä½“è®­ç»ƒé¢‘ç‡ï¼ˆæ¯å¤šå°‘æ­¥è®­ç»ƒä¸€æ¬¡ï¼‰
        self.train_freq = 100  # é»˜è®¤ä¸ºæ¯100æ­¥è®­ç»ƒä¸€æ¬¡

        # é«˜å±‚å’Œä½å±‚æ™ºèƒ½ä½“çš„æ‰¹é‡è®­ç»ƒå¤§å°
        self.H_BS = 100  # é«˜å±‚æ™ºèƒ½ä½“æ‰¹é‡è®­ç»ƒå¤§å°
        self.L_BS = 100  # ä½å±‚æ™ºèƒ½ä½“æ‰¹é‡è®­ç»ƒå¤§å°

        # åˆå§‹åŒ–ç¯å¢ƒ
        self.env = self._init_environment()

        # åˆ›å»ºç»éªŒç¼“å†²åŒº
        self._init_replay_buffers()

        # åˆå§‹åŒ–é«˜å±‚DQNå’Œä½å±‚TD3æ¨¡å‹
        self.high_level_agent = self._init_high_level_agent()  # é«˜å±‚å†³ç­–æ™ºèƒ½ä½“
        self.low_level_agent = self._init_low_level_agent()  # ä½å±‚æ‰§è¡Œæ™ºèƒ½ä½“

        # åˆ›å»ºå­˜å‚¨ç›®å½•
        self._create_directories()

        # â€”â€” æ³¨å†Œè¿›ç¨‹é€€å‡ºæ¸…ç†ï¼ˆæ— è®ºæ­£å¸¸ç»“æŸè¿˜æ˜¯å¼‚å¸¸é€€å‡º/æŒ‰ Ctrl+Cï¼‰
        atexit.register(self._cleanup_gazebo_ros)

    def _init_environment(self):
        """åˆå§‹åŒ–æœºå™¨äººå¯¼èˆªç¯å¢ƒã€‚

        è¯¥æ–¹æ³•åˆ›å»ºå¹¶è¿”å›ä¸€ä¸ªåŸºäºGazeboçš„Velodyneæ¿€å…‰é›·è¾¾æœºå™¨äººå¯¼èˆªç¯å¢ƒã€‚
        ä½¿ç”¨VelodyneGymWrapper(åœ¨gym_wrapper.pyä¸­)åŒ…è£…å™¨å°†Gazeboç¯å¢ƒè½¬æ¢ä¸ºç¬¦åˆOpenAI Gymæ¥å£çš„ç¯å¢ƒ, 
        ä»¥ä¾¿äºå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„ä½¿ç”¨ã€‚

        """
        print("æ­£åœ¨åˆå§‹åŒ–ç¯å¢ƒ...")
        env = VelodyneGymWrapper(
            launchfile="multi_robot_scenario.launch",  # Gazeboå¯åŠ¨æ–‡ä»¶
            environment_dim=self.environment_dim,  # é»˜è®¤20
            action_type="continuous",  # åŠ¨ä½œç±»å‹ä¸ºè¿ç»­ï¼Œé€‚åº”ä½å±‚TD3æ™ºèƒ½ä½“
            device=self.device  # è®¡ç®—è®¾å¤‡
        )
        print("ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ!")
        return env

    def _init_replay_buffers(self):
        """åˆå§‹åŒ–ç»éªŒå›æ”¾ç¼“å†²åŒº

        ç»éªŒå›æ”¾ç¼“å†²åŒºç”¨äºå­˜å‚¨æ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’äº§ç”Ÿçš„ç»éªŒï¼Œä»¥ä¾¿åç»­è®­ç»ƒä½¿ç”¨ã€‚
        è¯¥æ–¹æ³•ä¸ºé«˜å±‚å’Œä½å±‚æ™ºèƒ½ä½“åˆ†åˆ«åˆ›å»ºæ™®é€šç»éªŒå›æ”¾ç¼“å†²åŒºã€‚

        """
        print("æ­£åœ¨åˆå§‹åŒ–ç»éªŒå›æ”¾ç¼“å†²åŒº...")
        # è·å–çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ç»´åº¦
        # è·å–çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ç»´åº¦
        # é«˜å±‚çŠ¶æ€ç»´åº¦ä¸ç¯å¢ƒçš„è§‚å¯Ÿç©ºé—´ç»´åº¦ç›¸åŒ
        high_level_state_dim = self.env.observation_space.shape[0]
        num_directions = self.environment_dim
        num_distances = 10
        high_level_action_dim = num_directions * num_distances  # é«˜å±‚åŠ¨ä½œæ•°(æ–¹å‘æ•°é‡Ã—è·ç¦»çº§åˆ«)
        low_level_state_dim = high_level_state_dim + 2  # ä½å±‚çŠ¶æ€ç»´åº¦=é«˜å±‚çŠ¶æ€+æ–¹å‘+è·ç¦»
        low_level_action_dim = self.env.action_space.shape[0]  # ä½å±‚åŠ¨ä½œç»´åº¦

        # ä½¿ç”¨æ™®é€šç»éªŒå›æ”¾ç¼“å†²åŒº
        # ä¸ºé«˜å±‚å’Œä½å±‚æ™ºèƒ½ä½“åˆ›å»ºè§‚å¯Ÿç©ºé—´å’ŒåŠ¨ä½œç©ºé—´
        import numpy as np

        # é«˜å±‚æ™ºèƒ½ä½“çš„è§‚å¯Ÿç©ºé—´å’ŒåŠ¨ä½œç©ºé—´
        # æ ¹æ®gym_wrapper.pyä¸­çš„å®šä¹‰è®¾ç½®è§‚å¯Ÿç©ºé—´
        laser_low = 0.0
        laser_high = 10.0
        distance_low = 0.0
        distance_high = 7.0
        angle_low = -np.pi
        angle_high = np.pi
        vel_low = 0.0
        vel_high = 1.0
        ang_vel_low = -1.0
        ang_vel_high = 1.0

        high_level_observation_low = np.array([laser_low] * self.environment_dim + [distance_low, angle_low, vel_low, ang_vel_low], dtype=np.float32)
        high_level_observation_high = np.array([laser_high] * self.environment_dim + [distance_high, angle_high, vel_high, ang_vel_high], dtype=np.float32)

        high_level_observation_space = Box(low=high_level_observation_low, high=high_level_observation_high, dtype=np.float32)
        high_level_action_space = Discrete(high_level_action_dim)

        # ä½å±‚æ™ºèƒ½ä½“çš„è§‚å¯Ÿç©ºé—´å’ŒåŠ¨ä½œç©ºé—´
        # ä½å±‚æ™ºèƒ½ä½“çš„è§‚å¯Ÿç©ºé—´å’ŒåŠ¨ä½œç©ºé—´
        low_level_observation_low = np.concatenate([high_level_observation_low, [-np.pi, 0.0]]).astype(np.float32)
        low_level_observation_high = np.concatenate([high_level_observation_high, [np.pi, 10.0]]).astype(np.float32)
        low_level_observation_space = Box(low=low_level_observation_low, high=low_level_observation_high, dtype=np.float32)
        low_level_action_space = self.env.action_space  # ç›´æ¥ä½¿ç”¨ç¯å¢ƒçš„åŠ¨ä½œç©ºé—´

        print(f"ä½¿ç”¨{gym_lib}åº“å®šä¹‰è§‚å¯Ÿç©ºé—´ï¼Œdtype={np.float32}")

        # self.high_level_buffer = ReplayBuffer(  # é«˜å±‚ï¼ˆDQNï¼‰çš„ç»éªŒå›æ”¾ç¼“å†²åŒº
        #     buffer_size=1_000_000,
        #     observation_space=high_level_observation_space,
        #     action_space=high_level_action_space,
        #     device=self.device,
        #     # n_envs=1  # æ˜ç¡®æŒ‡å®šç¯å¢ƒæ•°é‡ä¸º1
        #     n_envs=getattr(self.env, "num_envs", 1)   # è‡ªåŠ¨é€‚é…ç¯å¢ƒæ•°
        # )
        # self.low_level_buffer = ReplayBuffer(  # ä½å±‚ï¼ˆTD3ï¼‰çš„ç»éªŒå›æ”¾ç¼“å†²åŒº
        #     buffer_size=1_000_000,
        #     observation_space=low_level_observation_space,
        #     action_space=low_level_action_space,
        #     device=self.device,
        #     # n_envs=1  # æ˜ç¡®æŒ‡å®šç¯å¢ƒæ•°é‡ä¸º1
        #     n_envs=getattr(self.env, "num_envs", 1)   # è‡ªåŠ¨é€‚é…ç¯å¢ƒæ•°
        # )
        # print("ä½¿ç”¨æ™®é€šç»éªŒå›æ”¾ç¼“å†²åŒº")

    def _init_high_level_agent(self):
        """åˆå§‹åŒ–é«˜å±‚æ™ºèƒ½ä½“

        é«˜å±‚æ™ºèƒ½ä½“ä½¿ç”¨DQNç®—æ³•, è´Ÿè´£å†³å®šæœºå™¨äººçš„å¯¼èˆªæ–¹å‘å’Œç§»åŠ¨è·ç¦»ã€‚DQNé€‚åˆå¤„ç†ç¦»æ•£åŠ¨ä½œç©ºé—´çš„é—®é¢˜ã€‚
        é«˜å±‚æ™ºèƒ½ä½“çš„åŠ¨ä½œç©ºé—´æ˜¯ç¦»æ•£çš„, å¯¹åº”20ä¸ªä¸åŒçš„æ–¹å‘ã€‚

        è¿”å›:
            åˆå§‹åŒ–åçš„DQN Agent
        """
        print("æ­£åœ¨åˆå§‹åŒ–é«˜å±‚DQNæ™ºèƒ½ä½“...")
        # ä¸ºé«˜å±‚DQNåˆ›å»ºä¸€ä¸ªå…·æœ‰ç¦»æ•£åŠ¨ä½œç©ºé—´çš„ç¯å¢ƒ
        import gymnasium
        from gymnasium import spaces
        from stable_baselines3.common.env_util import make_vec_env
        
        # é«˜å±‚åŠ¨ä½œç©ºé—´ï¼š20ä¸ªæ–¹å‘ Ã— 10ä¸ªè·ç¦»çº§åˆ« = 200ä¸ªç¦»æ•£åŠ¨ä½œ
        num_directions = 20
        num_distances = 10
        high_level_action_space = spaces.Discrete(num_directions * num_distances)
        
        # åˆ›å»ºä¸€ä¸ªç¯å¢ƒåŒ…è£…å™¨ï¼Œä½¿ç”¨ç¦»æ•£åŠ¨ä½œç©ºé—´
        class DiscreteActionEnvWrapper(gymnasium.Env):
            """
            ä»…ç”¨äºç»™ DQN æä¾›ä¸€ä¸ªâ€œåˆæ³•â€çš„ç¯å¢ƒæ¥å£ï¼Œé¿å… SB3 å†…éƒ¨è°ƒç”¨ reset/step æ—¶
            çœŸçš„å»é©±åŠ¨ Gazeboã€‚æˆ‘ä»¬å®Œå…¨ä¸ä¼šç”¨å®ƒæ¥ rolloutsï¼Œåªä¼šç”¨ high_level_agent.predict()ã€‚
            """
            def __init__(self, env):
                super().__init__()
                self.env = env
                self.observation_space = env.observation_space
                self.action_space = high_level_action_space
                self._last_obs = None

            def reset(self, seed=None, options=None):
                reset_result = self.env.reset(seed=seed, options=options)
                obs = reset_result[0] if isinstance(reset_result, tuple) else reset_result
                self._last_obs = obs
                # gymnasium éœ€è¦è¿”å› (obs, info)
                return obs, {}

            def step(self, action):
                # ä¸è¦åœ¨ step() é‡Œ reset ç¯å¢ƒï¼è¿”å›ä¸€ä¸ª no-op çš„ä¸€æ­¥å³å¯ã€‚
                assert self._last_obs is not None, "Call reset() before step()."
                return self._last_obs, 0.0, False, False, {}
            
            def render(self, mode='human'):
                return self.env.render(mode=mode)
            
            def close(self):
                return self.env.close()
        
        discrete_env = DiscreteActionEnvWrapper(self.env)
        
        model = DQN(
            "MlpPolicy", 
            discrete_env,
            verbose=0,  
            tensorboard_log="./logs/high_level_dqn",  # æ—¥å¿—ä¿å­˜è·¯å¾„
            learning_rate=1e-4,  # å­¦ä¹ ç‡
            buffer_size=1_000_000,  # ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°
            batch_size=64,  # æ‰¹é‡å¤§å°
            gamma=0.99,  # æŠ˜æ‰£å› å­
            device=self.device,  # è®¡ç®—è®¾å¤‡
            exploration_initial_eps=self.epsilon_start,  # åˆå§‹æ¢ç´¢ç‡
            exploration_final_eps=self.epsilon_end,  # æœ€ç»ˆæ¢ç´¢ç‡
            exploration_fraction=self.epsilon_decay / self.max_timesteps  # æ¢ç´¢ç‡è¡°å‡æ¯”ä¾‹
        )

        # æ·»åŠ æ¨¡å‹ä¿å­˜å›è°ƒï¼Œæ¯eval_freqæ­¥ä¿å­˜ä¸€æ¬¡
        self.high_level_checkpoint_callback = CheckpointCallback(
            save_freq=self.eval_freq,
            save_path="./pytorch_models/high_level/",
            name_prefix="high_level_dqn",
        )

        # æ·»åŠ æ—¥å¿—è®°å½•å›è°ƒï¼Œæ¯eval_freqæ­¥è®°å½•ä¸€æ¬¡è¯¦ç»†æ—¥å¿—
        self.high_level_log_callback = LoggingCallback(eval_freq=self.eval_freq, verbose=1)

        print("é«˜å±‚DQNæ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ!")
        return model

    def _init_low_level_agent(self):
        """åˆå§‹åŒ–ä½å±‚TD3æ™ºèƒ½ä½“

        ä½å±‚æ™ºèƒ½ä½“ä½¿ç”¨åŒå»¶è¿Ÿæ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦(TD3)ç®—æ³•, è´Ÿè´£æ‰§è¡Œé«˜å±‚æ™ºèƒ½ä½“åˆ¶å®šçš„å­ç›®æ ‡ã€‚
        TD3é€‚åˆå¤„ç†è¿ç»­åŠ¨ä½œç©ºé—´çš„é—®é¢˜ã€‚ä½å±‚æ™ºèƒ½ä½“éœ€è¦æ§åˆ¶æœºå™¨äººçš„å…·ä½“è¿åŠ¨, åŠ¨ä½œç©ºé—´æ˜¯è¿ç»­çš„ã€‚

        è¿”å›:
            åˆå§‹åŒ–åçš„TD3 Agent
        """
        print("æ­£åœ¨åˆå§‹åŒ–ä½å±‚TD3æ™ºèƒ½ä½“...")
        # ä½å±‚TD3å¤„ç†è¿ç»­åŠ¨ä½œ
        # åˆ›å»ºä¸€ä¸ªæ–°çš„è§‚å¯Ÿç©ºé—´ï¼ŒåŒ…å«åŸå§‹çŠ¶æ€(24ç»´)åŠ ä¸Šæ–¹å‘å’Œè·ç¦»(2ç»´)
        low = np.append(self.env.observation_space.low, [-np.pi, 0.0]).astype(np.float32)  # æ–¹å‘èŒƒå›´[-pi, pi]ï¼Œè·ç¦»èŒƒå›´[0.0, 5.0]
        high = np.append(self.env.observation_space.high, [np.pi, 5.0]).astype(np.float32)
        extended_observation_space = Box(low=low, high=high, dtype=np.float32)

        # ä½¿ç”¨æ‰©å±•åçš„è§‚å¯Ÿç©ºé—´åˆ›å»ºä¸€ä¸ªåŒ…è£…ç¯å¢ƒ
        if gym_lib == 'gymnasium':
            class ExtendedObservationEnvWrapper(gymnasium.Wrapper):
                def __init__(self, env):
                    super().__init__(env)
                    self.observation_space = extended_observation_space
        else:  # gym_lib == 'gym'
            class ExtendedObservationEnvWrapper(gym.Wrapper):
                def __init__(self, env):
                    super().__init__(env)
                    self.observation_space = extended_observation_space

        extended_env = ExtendedObservationEnvWrapper(self.env)

        model = TD3(
            "MlpPolicy", 
            extended_env,  
            verbose=0, 
            tensorboard_log="./logs/low_level_td3",  # æ—¥å¿—ä¿å­˜è·¯å¾„
            learning_rate=1e-4,  # å­¦ä¹ ç‡
            buffer_size=1_000_000,  # ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°
            batch_size=40,  # ä¸velodyneç‰ˆæœ¬ä¸€è‡´
            tau=0.005,  # ç›®æ ‡ç½‘ç»œæ›´æ–°ç³»æ•°
            gamma=0.99999,  # ä¸velodyneç‰ˆæœ¬ä¸€è‡´
            train_freq=1,  # è®­ç»ƒé¢‘ç‡
            gradient_steps=1,  # æ¯æ¬¡è®­ç»ƒçš„æ¢¯åº¦æ­¥æ•°
            policy_delay=2,  # ç­–ç•¥æ›´æ–°å»¶è¿Ÿ
            target_policy_noise=0.2,  # ç›®æ ‡ç­–ç•¥å™ªå£°
            target_noise_clip=0.5,  # å™ªå£°è£å‰ª
            device=self.device  # è®¡ç®—è®¾å¤‡
        )

        # æ·»åŠ æ¨¡å‹ä¿å­˜å›è°ƒï¼Œæ¯eval_freqæ­¥ä¿å­˜ä¸€æ¬¡
        self.low_level_checkpoint_callback = CheckpointCallback(
            save_freq=self.eval_freq,
            save_path="./pytorch_models/low_level/",
            name_prefix="low_level_td3",
        )

        # æ·»åŠ æ—¥å¿—è®°å½•å›è°ƒï¼Œæ¯eval_freqæ­¥è®°å½•ä¸€æ¬¡è¯¦ç»†æ—¥å¿—
        self.low_level_log_callback = LoggingCallback(eval_freq=self.eval_freq, verbose=1)

        print("ä½å±‚TD3æ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ!")
        return model

    def _create_directories(self):
        """åˆ›å»ºæ¨¡å‹å’Œç»“æœå­˜å‚¨ç›®å½•

        åˆ›å»ºç”¨äºå­˜å‚¨è®­ç»ƒç»“æœã€æ¨¡å‹æƒé‡çš„ç›®å½•ç»“æ„ï¼Œç¡®ä¿è®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆçš„æ–‡ä»¶æœ‰åˆé€‚çš„å­˜æ”¾ä½ç½®ã€‚å¦‚æœç›®å½•å·²å­˜åœ¨ï¼Œåˆ™ä¸ä¼šé‡å¤åˆ›å»ºã€‚
        """
        directories = [
            "./results", 
            "./pytorch_models/high_level", 
            "./pytorch_models/low_level",
            "./logs/high_level_dqn",
            "./logs/low_level_td3"
        ]
        for dir_path in directories:
            if not os.path.exists(dir_path):
                os.makedirs(dir_path, exist_ok=True)
                print(f"åˆ›å»ºç›®å½•: {dir_path}")

    def _prepare_sb3_train(self, agent):
        """
        è®© SB3 æ™ºèƒ½ä½“åœ¨ä¸èµ° learn() çš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½å®‰å…¨è°ƒç”¨ train()ã€‚
        - åˆ›å»º logger
        - è®¾ç½®è¿›åº¦å˜é‡
        - è¡¥é½ç»Ÿè®¡è®¡æ•°å™¨
        """
        # 1) loggerï¼ˆSB3 çš„ train() ä¼šç”¨åˆ°ï¼‰
        if getattr(agent, "_logger", None) is None:
            from stable_baselines3.common.logger import configure
            # ç¡®å®šTensorBoardæ—¥å¿—è·¯å¾„
            if hasattr(agent, 'tensorboard_log') and agent.tensorboard_log:
                log_folder = agent.tensorboard_log
            else:
                log_folder = "./logs/default"
            
            # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
            os.makedirs(log_folder, exist_ok=True)
            agent._logger = configure(folder=log_folder, format_strings=["stdout", "tensorboard"])

        # 2) è¿›åº¦å˜é‡ï¼ˆç”¨äºå­¦ä¹ ç‡è°ƒåº¦ï¼‰
        if not hasattr(agent, "_current_progress_remaining"):
            agent._current_progress_remaining = 1.0  # ç›¸å½“äºè®­ç»ƒåˆšå¼€å§‹

        # 3) è®¡æ•°å™¨ï¼ˆæœ‰äº›ç®—æ³•åœ¨æ—¥å¿—é‡Œä¼šç”¨åˆ°ï¼‰
        if not hasattr(agent, "_n_updates"):
            agent._n_updates = 0
        if not hasattr(agent, "num_timesteps"):
            agent.num_timesteps = 0

        # 4) å…œåº•ï¼šæœ‰äº›ç‰ˆæœ¬æŠŠ lr_schedule æŒ‚åœ¨ agent ä¸Šï¼›é€šå¸¸å·²å­˜åœ¨ï¼Œè¿™é‡Œé˜²å¾¡æ€§è¡¥ä¸€ä¸‹
        if not hasattr(agent, "lr_schedule"):
            # å°½é‡ç”¨ agent.learning_rateï¼Œç¼ºçœ 3e-4
            base_lr = float(getattr(agent, "learning_rate", 3e-4))
            agent.lr_schedule = lambda progress: base_lr * 1.0  # å›ºå®šå­¦ä¹ ç‡

    def load_high_level_model(self, model_path):
        """åŠ è½½é«˜å±‚DQNæ¨¡å‹

        è¯¥æ–¹æ³•å°è¯•ä»æŒ‡å®šè·¯å¾„åŠ è½½é¢„è®­ç»ƒçš„é«˜å±‚DQNæ¨¡å‹ã€‚é«˜å±‚æ¨¡å‹è´Ÿè´£å†³ç­–æœºå™¨äººçš„å¯¼èˆªæ–¹å‘å’Œç§»åŠ¨è·ç¦»ã€‚
        å¦‚æœåŠ è½½æˆåŠŸ, å°†ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œåç»­æ“ä½œ; å¦‚æœåŠ è½½å¤±è´¥, å°†æ‰“å°é”™è¯¯ä¿¡æ¯ä½†ä¸ä¼šä¸­æ–­ç¨‹åºã€‚

        å‚æ•°:
            model_path: é¢„è®­ç»ƒæ¨¡å‹çš„æ–‡ä»¶è·¯å¾„
        """
        try:
            self.high_level_agent = DQN.load(model_path, env=self.env, device=self.device)
            print(f"æˆåŠŸåŠ è½½é«˜å±‚æ¨¡å‹: {model_path}")
        except Exception as e:
            print(f"åŠ è½½é«˜å±‚æ¨¡å‹å¤±è´¥: {e}")

    def load_low_level_model(self, model_path):
        """åŠ è½½ä½å±‚TD3æ¨¡å‹

        è¯¥æ–¹æ³•å°è¯•ä»æŒ‡å®šè·¯å¾„åŠ è½½é¢„è®­ç»ƒçš„ä½å±‚TD3æ¨¡å‹ã€‚ä½å±‚æ¨¡å‹è´Ÿè´£æ‰§è¡Œé«˜å±‚æ¨¡å‹åˆ¶å®šçš„å­ç›®æ ‡, æ§åˆ¶æœºå™¨äººçš„å…·ä½“è¿åŠ¨ã€‚
        å¦‚æœåŠ è½½æˆåŠŸ, å°†ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œåç»­æ“ä½œ;å¦‚æœåŠ è½½å¤±è´¥, å°†æ‰“å°é”™è¯¯ä¿¡æ¯ä½†ä¸ä¼šä¸­æ–­ç¨‹åºã€‚

        å‚æ•°:
            model_path: é¢„è®­ç»ƒæ¨¡å‹çš„æ–‡ä»¶è·¯å¾„
        """
        try:
            self.low_level_agent = TD3.load(model_path, env=self.env, device=self.device)
            print(f"æˆåŠŸåŠ è½½ä½å±‚æ¨¡å‹: {model_path}")
        except Exception as e:
            print(f"åŠ è½½ä½å±‚æ¨¡å‹å¤±è´¥: {e}")


    def _update_exploration_parameters(self, timestep: int):
        """æ›´æ–°æ¢ç´¢ç­–ç•¥å‚æ•°

        å…è®¸æ™ºèƒ½ä½“åœ¨æœªçŸ¥ç¯å¢ƒä¸­å°è¯•ä¸åŒçš„åŠ¨ä½œæ¢ç´¢ä»¥å‘ç°æœ€ä¼˜ç­–ç•¥ã€‚è¯¥æ–¹æ³•æ ¹æ®è®­ç»ƒæ­¥æ•°åŠ¨æ€è°ƒæ•´æ¢ç´¢å‚æ•°ï¼š
        - å¯¹äºDQN(é«˜å±‚æ™ºèƒ½ä½“), ä½¿ç”¨epsilon-greedyç­–ç•¥, éšç€è®­ç»ƒè¿›è¡Œ, æ¢ç´¢ç‡é€æ¸é™ä½ï¼›
        - å¯¹äºTD3(ä½å±‚æ™ºèƒ½ä½“), ä½¿ç”¨å™ªå£°æ¢ç´¢ç­–ç•¥, éšç€è®­ç»ƒè¿›è¡Œ, å™ªå£°å¼ºåº¦é€æ¸é™ä½ã€‚

        å‚æ•°:
            timestep: å½“å‰è®­ç»ƒæ­¥æ•°
        """
        # æ›´æ–°epsilon (DQN) - æŒ‡æ•°è¡°å‡
        self.current_epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \
            np.exp(-1. * timestep / self.epsilon_decay)

        # æ›´æ–°å™ªå£°å‚æ•° (TD3) - æŒ‡æ•°è¡°å‡
        self.current_noise = self.noise_end + (self.noise_start - self.noise_end) * \
            np.exp(-1. * timestep / self.noise_decay)

        # åº”ç”¨åˆ°DQNæ¨¡å‹
        if hasattr(self.high_level_agent, 'exploration_rate'):
            self.high_level_agent.exploration_rate = self.current_epsilon
        elif hasattr(self.high_level_agent, 'policy') and hasattr(self.high_level_agent.policy, 'exploration_rate'):
            self.high_level_agent.policy.exploration_rate = self.current_epsilon
        elif hasattr(self.high_level_agent, 'policy') and hasattr(self.high_level_agent.policy, 'epsilon'):
            self.high_level_agent.policy.epsilon = self.current_epsilon

        # åº”ç”¨åˆ°TD3æ¨¡å‹
        if hasattr(self.low_level_agent, 'policy') and hasattr(self.low_level_agent.policy, 'target_policy_noise'): # å¦‚æœæœ‰è¿™ä¸ªå‚æ•°çš„è¯
            self.low_level_agent.policy.target_policy_noise = self.current_noise # æ›´æ–°ä¸ºcurrent_noise

    def _calculate_rewards(self, state: np.ndarray, next_state: np.ndarray, action: int, distance: float,
                          done: bool, target: bool, episode_timesteps: int, reward: float, info: dict) -> Tuple[float, float]:
        # ç¡®ä¿prev_directionå·²å®šä¹‰
        if not hasattr(self, 'prev_direction'):
            self.prev_direction = 0.0
        """è®¡ç®—é«˜å±‚å’Œä½å±‚æ™ºèƒ½ä½“çš„å¥–åŠ±

        å¥–åŠ±å‡½æ•°æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­çš„å…³é”®ç»„ä»¶ï¼Œå®ƒå®šä¹‰äº†æ™ºèƒ½ä½“è¡Œä¸ºçš„å¥½åã€‚è¯¥æ–¹æ³•è®¡ç®—
        å¤šç§å¥–åŠ±æˆåˆ†ï¼ŒåŒ…æ‹¬æ–¹å‘å¥–åŠ±ã€è·ç¦»å¥–åŠ±ã€éšœç¢ç‰©è§„é¿å¥–åŠ±å’Œè·¯å¾„å¹³æ»‘æ€§å¥–åŠ±ï¼Œ
        å¹¶å°†å®ƒä»¬ç»„åˆæˆæ€»å¥–åŠ±ã€‚é«˜å±‚å’Œä½å±‚æ™ºèƒ½ä½“æœ‰ä¸åŒçš„å¥–åŠ±è®¡ç®—æ–¹å¼ã€‚

        å‚æ•°:
            state: å½“å‰çŠ¶æ€
            next_state: ä¸‹ä¸€ä¸ªçŠ¶æ€
            action: é«˜å±‚æ™ºèƒ½ä½“é€‰æ‹©çš„åŠ¨ä½œ
            distance: é«˜å±‚æ™ºèƒ½ä½“é€‰æ‹©çš„è·ç¦»
            done: æ˜¯å¦ç»“æŸå›åˆ
            target: æ˜¯å¦åˆ°è¾¾ç›®æ ‡
            episode_timesteps: å½“å‰å›åˆæ­¥æ•°
            reward: ç¯å¢ƒåŸå§‹å¥–åŠ±

        è¿”å›:
            Tuple[float, float]: é«˜å±‚æ™ºèƒ½ä½“å¥–åŠ±å’Œä½å±‚æ™ºèƒ½ä½“å¥–åŠ±
        """
        # æå–ä½ç½®ä¿¡æ¯
        current_position = state[:2]
        next_position = next_state[:2]
        direction = action % self.environment_dim

        # è®¡ç®—å®é™…ç§»åŠ¨
        dx = next_position[0] - current_position[0]
        dy = next_position[1] - current_position[1]
        actual_distance = np.sqrt(dx**2 + dy**2)
        actual_direction = np.arctan2(dy, dx) * 180 / np.pi % 360

        # ç›®æ ‡æ–¹å‘
        target_direction = (direction * 360 / self.environment_dim) % 360

        # æ–¹å‘å’Œè·ç¦»å·®å¼‚
        direction_diff = min(abs(actual_direction - target_direction), 360 - abs(actual_direction - target_direction))
        distance_diff = abs(actual_distance - distance)

        # é«˜å±‚å¥–åŠ±ç»„æˆ
        direction_reward = 1.0 - (direction_diff / 180.0)
        distance_reward = 1.0 - min(distance_diff / distance, 1.0)
        collision_penalty = -1.0 if done and not target else 0.0
        time_penalty = -0.01

        # æ–°å¢: å­ç›®æ ‡åˆç†æ€§å¥–åŠ±
        # æ£€æŸ¥ç›®æ ‡æ–¹å‘æ˜¯å¦æœ‰éšœç¢ç‰©
        obstacle_avoidance_reward = 0.0
        # æ£€æŸ¥ç›®æ ‡æ–¹å‘æ˜¯å¦æœ‰éšœç¢ç‰©
        obstacle_avoidance_reward = 0.2 if 'obstacle_ahead' in info and not info['obstacle_ahead'] else 0.0

        # æ–°å¢: è·¯å¾„è´¨é‡å¥–åŠ± (å¹³æ»‘æ€§)
        if episode_timesteps > 0 and hasattr(self, 'prev_direction'):
            direction_change = min(abs(actual_direction - self.prev_direction), 360 - abs(actual_direction - self.prev_direction))
            smoothness_reward = 0.1 * (1.0 - min(direction_change / 90.0, 1.0))
        else:
            smoothness_reward = 0.0

        # æ›´æ–°å‰ä¸€æ–¹å‘
        self.prev_direction = actual_direction

        # ç»¼åˆé«˜å±‚å¥–åŠ± = æ–¹å‘å¥–åŠ± * 0.4 + è·ç¦»å¥–åŠ± * 0.4 + éšœç¢ç‰©å¥–åŠ± * 0.1 + å¹³æ»‘å¥–åŠ± * 0.1 + ç¢°æ’å¥–åŠ± + æ—¶é—´å¥–åŠ±
        high_level_reward = (direction_reward * 0.4 + distance_reward * 0.4 + 
                            obstacle_avoidance_reward * 0.1 + smoothness_reward * 0.1 + 
                            collision_penalty + time_penalty)
        high_level_reward = max(high_level_reward, -2.0)

        # ä½å±‚å¥–åŠ±: ç»“åˆç¯å¢ƒåé¦ˆå’Œå­ç›®æ ‡å®Œæˆåº¦
        low_level_reward = reward  # åŸå§‹ç¯å¢ƒå¥–åŠ±
        low_level_reward += 0.5 * (direction_reward + distance_reward)  # å­ç›®æ ‡å®Œæˆåº¦å¥–åŠ±
        low_level_reward += collision_penalty * 0.5  # ç¢°æ’æƒ©ç½š

        return high_level_reward, low_level_reward

    def train(self, log_interval=1000):
        """åˆ†å±‚å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼ˆå¤–éƒ¨é‡‡æ · â†’ å†…éƒ¨ buffer â†’ ä»…æ¢¯åº¦æ›´æ–°ï¼‰"""
        print("å¼€å§‹å±‚çº§å¼ºåŒ–å­¦ä¹ è®­ç»ƒ...")
        
        # ç¡®ä¿æ‰€æœ‰å¿…è¦çš„ç›®å½•éƒ½å­˜åœ¨
        self._create_directories()

        self._prepare_sb3_train(self.high_level_agent)
        self._prepare_sb3_train(self.low_level_agent)

        timestep = 0
        evaluations = []
        self.prev_direction = 0.0  # ç”¨äºå¹³æ»‘å¥–åŠ±
        episode_count = 0  # å›åˆè®¡æ•°å™¨ï¼Œä»1å¼€å§‹

        # æ–¹ä¾¿ä¹¦å†™
        H_BUF = self.high_level_agent.replay_buffer
        L_BUF = self.low_level_agent.replay_buffer
        H_BS  = self.high_level_agent.batch_size
        L_BS  = self.low_level_agent.batch_size

        # è®¡ç®—æ€»å›åˆæ•°
        total_episodes = int(self.max_timesteps / self.max_ep) + 1
        print(f"è®­ç»ƒè®¾ç½®:")
        print(f"  - æ€»æ—¶é—´æ­¥æ•°: {int(self.max_timesteps):,}")
        print(f"  - æ€»å›åˆæ•°: {total_episodes}")
        print(f"  - ä¿å­˜é¢‘ç‡: æ¯ {self.eval_freq} æ­¥ä¿å­˜ä¸€æ¬¡")
        print(f"  - é¢„è®¡ä¿å­˜æ¬¡æ•°: {int(self.max_timesteps / self.eval_freq)} æ¬¡")
        print(f"  - è°ƒè¯•ä¿¡æ¯: max_timesteps={self.max_timesteps}, eval_freq={self.eval_freq}")

        # åˆ›å»ºè¿›åº¦æ¡
        with tqdm(total=self.max_timesteps, desc="è®­ç»ƒè¿›åº¦", position=1, leave=True, dynamic_ncols=True) as pbar:
            while timestep < self.max_timesteps:
                # æ–°å›åˆå¼€å§‹
                episode_count += 1
                reset_result = self.env.reset()
                state = reset_result[0] if isinstance(reset_result, tuple) else reset_result
                # ç¡®ä¿ state æ˜¯ numpy array
                if isinstance(state, tuple):
                    state = state[0]
                state = np.array(state, dtype=np.float32)
                done = False
                episode_reward = 0.0
                episode_timesteps = 0
                pbar.set_postfix({"å›åˆ": episode_count, "å½“å‰å¥–åŠ±": f"{episode_reward:.2f}"})

                while not done and episode_timesteps < self.max_ep:
                    # æ¢ç´¢å‚æ•°æ›´æ–°ï¼ˆepsilon / å™ªå£°ï¼‰
                    self._update_exploration_parameters(timestep)

                    # ===== é«˜å±‚ï¼šç¦»æ•£ action -> (direction, distance) =====
                    # è¿™é‡Œåªç”¨ policy çš„ predictï¼Œä¸è®© SB3 è‡ªå·± rollouts
                    high_level_action = self.high_level_agent.predict(state, deterministic=False)[0]
                    direction = high_level_action % 20
                    distance  = (high_level_action // 20) * 0.5 + 0.5  # 0.5 ~ 5.0

                    # ===== ä½å±‚ï¼šè¿ç»­åŠ¨ä½œï¼ˆæŠŠå­ç›®æ ‡æ‹¼è¿›è§‚æµ‹ï¼‰=====
                    sub_goal_state = np.append(state, [direction, distance])
                    low_level_action = self.low_level_agent.predict(sub_goal_state, deterministic=False)[0]

                    # ä¸ Gazebo çœŸæ­£äº¤äº’çš„ä¸€æ­¥
                    next_state, env_reward, terminated, truncated, info = self.env.step(low_level_action)
                    # ç¡®ä¿ next_state æ˜¯ numpy array
                    if isinstance(next_state, tuple):
                        next_state = next_state[0]
                    next_state = np.array(next_state, dtype=np.float32)
                    done = terminated or truncated
                    target = info.get('target_reached', False) if info else False

                    # è®¡ç®—è‡ªå®šä¹‰å¥–åŠ±
                    high_level_reward, low_level_reward = self._calculate_rewards(
                        state=state, 
                        next_state=next_state, 
                        action=high_level_action, 
                        distance=distance, 
                        done=done, 
                        target=target, 
                        episode_timesteps=episode_timesteps, 
                        reward=env_reward, 
                        info=info
                    )

                    # æ›´æ–°å›åˆå¥–åŠ±ï¼ˆä½¿ç”¨é«˜å±‚å¥–åŠ±ï¼‰
                    episode_reward += high_level_reward

                    # é«˜å±‚ç»éªŒï¼š(s, a_high, R)ï¼ŒRæ˜¯å›åˆæ€»å¥–åŠ±ï¼ˆå»¶è¿Ÿæ›´æ–°ï¼‰
                    # ä½å±‚ç»éªŒï¼š(s', a_low, r)ï¼Œræ˜¯å•æ­¥å¥–åŠ±
                    # æ³¨æ„ï¼šé«˜å±‚å¥–åŠ±å°†åœ¨å›åˆç»“æŸæ—¶æ›´æ–°
                    # Add experience to replay buffers with correct parameter order for custom ReplayBuffer
                    # Format experience for SB3 ReplayBuffer
                    obs_h = state.reshape(1, -1)
                    next_obs_h = next_state.reshape(1, -1)
                    act_h = np.array([[high_level_action]], dtype=np.int64)
                    rew_h = np.array([0.0], dtype=np.float32)
                    done_h = np.array([bool(done)], dtype=np.bool_)
                    H_BUF.add(obs_h, next_obs_h, act_h, rew_h, done_h, infos=[{}])

                    obs_l = sub_goal_state.reshape(1, -1)
                    next_obs_l = np.append(next_state, [direction, distance]).reshape(1, -1)
                    act_l = np.array(low_level_action, dtype=np.float32).reshape(1, -1)
                    rew_l = np.array([low_level_reward], dtype=np.float32)
                    done_l = np.array([bool(done)], dtype=np.bool_)
                    L_BUF.add(obs_l, next_obs_l, act_l, rew_l, done_l, infos=[{}])

                    # é«˜çº§æ™ºèƒ½ä½“çš„æ›´æ–°æ¡ä»¶ï¼šæ¯ N æ­¥ AND ç¼“å†²åŒºè¶³å¤Ÿå¤§
                    if H_BUF.size() > self.learn_starts and timestep % self.train_freq == 0:
                        self.high_level_agent.train(batch_size=self.H_BS, gradient_steps=1)
                        # è®­ç»ƒæ—¶ä¸å•ç‹¬è®°å½•æ—¥å¿—ï¼Œç»Ÿä¸€ç”±ä¸‹é¢çš„æ¯100æ­¥è®°å½•

                    # ä½çº§æ™ºèƒ½ä½“çš„æ›´æ–°æ¡ä»¶ï¼šæ¯ N æ­¥ AND ç¼“å†²åŒºè¶³å¤Ÿå¤§
                    if L_BUF.size() > self.learn_starts and timestep % self.train_freq == 0:
                        self.low_level_agent.train(batch_size=self.L_BS, gradient_steps=1)
                        # è®­ç»ƒæ—¶ä¸å•ç‹¬è®°å½•æ—¥å¿—ï¼Œç»Ÿä¸€ç”±ä¸‹é¢çš„æ¯100æ­¥è®°å½•

                    # çŠ¶æ€è½¬ç§»
                    state = next_state
                    timestep += 1
                    episode_timesteps += 1

                    # æ›´æ–°è¿›åº¦æ¡
                    pbar.update(1)
                    pbar.set_postfix({"å›åˆ": episode_count, "å½“å‰å¥–åŠ±": f"{episode_reward:.2f}"})

                    # å®šæœŸè®°å½•è®­ç»ƒæŒ‡æ ‡åˆ°TensorBoardï¼ˆæ¯5000æ­¥è®°å½•ä¸€æ¬¡ï¼Œè¿›ä¸€æ­¥å‡å°‘æ—¥å¿—è¾“å‡ºï¼‰
                    if timestep % 5000 == 0:
                        self._log_training_metrics_to_tensorboard("high_level", timestep, episode_reward)
                        self._log_training_metrics_to_tensorboard("low_level", timestep, episode_reward)

                    # è¯„ä¼° + å®šæœŸæ‰‹åŠ¨ä¿å­˜
                    if timestep % self.eval_freq == 0:
                        print(f"\n=== è§¦å‘è¯„ä¼°å’Œä¿å­˜ (timestep={timestep}, eval_freq={self.eval_freq}) ===")
                        try:
                            # æ ¹æ®è®­ç»ƒè¿›åº¦åŠ¨æ€è°ƒæ•´è¯„ä¼°å›åˆæ•°
                            progress = timestep / self.max_timesteps
                            if progress < 0.1:  # å‰10%çš„è®­ç»ƒ
                                eval_episodes = 3
                            elif progress < 0.5:  # 10%-50%çš„è®­ç»ƒ
                                eval_episodes = 5
                            else:  # 50%ä»¥åçš„è®­ç»ƒ
                                eval_episodes = 10
                            
                            print(f"è¯„ä¼°å›åˆæ•°: {eval_episodes} (è®­ç»ƒè¿›åº¦: {progress:.1%})")
                            try:
                                # åªä½¿ç”¨è¯¦ç»†è¯„ä¼°ï¼Œä¸å†ä½¿ç”¨åŸºç¡€è¯„ä¼°
                                detailed_summary = self.evaluate_detailed(eval_episodes=eval_episodes)
                                # ä½¿ç”¨è¯¦ç»†è¯„ä¼°ç»“æœä¸­çš„å¹³å‡å¥–åŠ±ç”¨äºè®­ç»ƒè¿‡ç¨‹è·Ÿè¸ª
                                avg_reward = detailed_summary['avg_reward'] if 'avg_reward' in detailed_summary else 0.0
                                evaluations.append(avg_reward)
                                # ä¸å†å•ç‹¬ä¿å­˜evaluationsï¼Œæ‰€æœ‰è¯„ä¼°ç»“æœå·²é€šè¿‡evaluate_detailedä¿å­˜åˆ°logsæ–‡ä»¶å¤¹
                            except Exception as e:
                                print(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                                print("è·³è¿‡æœ¬æ¬¡è¯„ä¼°ï¼Œç»§ç»­è®­ç»ƒ...")
                                # ä½¿ç”¨é»˜è®¤å€¼
                                eval_reward = 0.0
                                evaluations.append(eval_reward)
                            # ç¡®ä¿ç›®å½•å­˜åœ¨
                            os.makedirs("./pytorch_models/high_level", exist_ok=True)
                            os.makedirs("./pytorch_models/low_level", exist_ok=True)
                            # ä¿å­˜æ¨¡å‹ï¼ˆéœ€è¦å®Œæ•´çš„æ–‡ä»¶è·¯å¾„ï¼‰
                            self.high_level_agent.save(f"./pytorch_models/high_level/ckpt_{timestep}")
                            self.low_level_agent.save(f"./pytorch_models/low_level/ckpt_{timestep}")
                            print(f"å·²ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹: timestep={timestep}")
                        except Exception as e:
                            print(f"è¯„ä¼°æˆ–ä¿å­˜è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                            # å³ä½¿è¯„ä¼°å¤±è´¥ï¼Œä¹Ÿç»§ç»­è®­ç»ƒ

                # å›åˆç»“æŸæ—¶æ›´æ–°é«˜å±‚ç»éªŒçš„å¥–åŠ±å€¼
                if done:
                    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æ‰¾åˆ°è¯¥å›åˆçš„æ‰€æœ‰ç»éªŒå¹¶æ›´æ–°
                    # ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬åªæ›´æ–°æœ€åä¸€æ¡ç»éªŒçš„å¥–åŠ±
                    if H_BUF.size() > 0:
                        # ç¡®ä¿last_idxæ˜¯æœ‰æ•ˆçš„
                        last_idx = H_BUF.size() - 1
                        # æ›´æ–°å¥–åŠ±ä¸ºå›åˆæ€»å¥–åŠ±
                        H_BUF.rewards[last_idx] = episode_reward

                    # ------- å›åˆç»“æŸåï¼Œç”¨"å‰©ä½™ç»éªŒ"å†å¤šåšä¸€äº›æ¢¯åº¦æ­¥ -------
                    if H_BUF.size() > 0 or L_BUF.size() > 0:
                        # ä»…å½“æ ·æœ¬ >= batch_size æ‰è®­ç»ƒï¼Œé¿å… SB3 é‡‡æ ·æŠ¥é”™
                        if H_BUF.size() >= self.H_BS:
                            steps_h = max(1, H_BUF.size() // 2)
                            self._prepare_sb3_train(self.high_level_agent)
                            self.high_level_agent.train(gradient_steps=steps_h, batch_size=self.H_BS)

                        if L_BUF.size() >= self.L_BS:
                            steps_l = max(1, L_BUF.size() // 2)
                            self._prepare_sb3_train(self.low_level_agent)
                            self.low_level_agent.train(gradient_steps=steps_l, batch_size=self.L_BS)


        # æœ€ç»ˆä¿å­˜
        os.makedirs("./pytorch_models/high_level", exist_ok=True)
        os.makedirs("./pytorch_models/low_level", exist_ok=True)
        self.high_level_agent.save("./pytorch_models/high_level/final_model")
        self.low_level_agent.save("./pytorch_models/low_level/final_model")
        print("è®­ç»ƒå®Œæˆ! æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜ã€‚")

    @staticmethod
    def objective(trial: optuna.Trial) -> float:
        """Optunaè¶…å‚æ•°ä¼˜åŒ–çš„ç›®æ ‡å‡½æ•°

        è¯¥æ–¹æ³•å®šä¹‰äº†è¶…å‚æ•°æœç´¢ç©ºé—´ï¼Œå¹¶è¿”å›æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„æ€§èƒ½æŒ‡æ ‡ï¼Œä¾›Optuna
        ä¼˜åŒ–å™¨å¯»æ‰¾æœ€ä¼˜è¶…å‚æ•°ç»„åˆã€‚è¶…å‚æ•°ä¼˜åŒ–æ˜¯æé«˜å¼ºåŒ–å­¦ä¹ æ€§èƒ½çš„å…³é”®æ­¥éª¤ï¼Œ
        é€šè¿‡å°è¯•ä¸åŒçš„å‚æ•°ç»„åˆï¼Œå¯ä»¥æ‰¾åˆ°æœ€é€‚åˆå½“å‰ä»»åŠ¡çš„è®¾ç½®ã€‚

        å‚æ•°:
            trial: Optuna Trialå¯¹è±¡ï¼Œç”¨äºé‡‡æ ·è¶…å‚æ•°

        è¿”å›:
            float: æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„å¹³å‡å¥–åŠ±ï¼Œä½œä¸ºä¼˜åŒ–ç›®æ ‡
        """
        # å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´
        environment_dim = 20
        max_timesteps = 1e6  # ä¸ºäº†å¿«é€Ÿä¼˜åŒ–ï¼Œä½¿ç”¨è¾ƒå°çš„è®­ç»ƒæ­¥æ•°
        eval_freq = 1000

        # è¶…å‚æ•°æœç´¢ç©ºé—´
        batch_train_size = trial.suggest_int("batch_train_size", 32, 256, step=32)
        high_level_lr = trial.suggest_float("high_level_lr", 1e-5, 1e-3, log=True)
        low_level_lr = trial.suggest_float("low_level_lr", 1e-5, 1e-3, log=True)
        gamma_high = trial.suggest_float("gamma_high", 0.9, 0.999)
        gamma_low = trial.suggest_float("gamma_low", 0.99, 0.99999)
        epsilon_decay = trial.suggest_int("epsilon_decay", 5e4, 2e5)
        noise_decay = trial.suggest_int("noise_decay", 5e4, 2e5)

        # åˆ›å»ºå±‚çº§RLå®ä¾‹
        agent = HierarchicalRL(
            environment_dim=environment_dim,
            max_timesteps=max_timesteps,
            eval_freq=eval_freq,
            batch_train_size=batch_train_size,
            epsilon_decay=epsilon_decay,
            noise_decay=noise_decay
        )

        # æ³¨æ„: ç›´æ¥ä¿®æ”¹å·²åˆå§‹åŒ–æ¨¡å‹çš„å‚æ•°ä¸æ˜¯æœ€ä½³å®è·µ
        # æ›´å¥½çš„æ–¹å¼æ˜¯åœ¨åˆ›å»ºæ¨¡å‹æ—¶å°±è®¾ç½®è¿™äº›å‚æ•°
        # è¿™é‡Œä¸ºäº†å…¼å®¹Optunaä¼˜åŒ–æµç¨‹è€Œä¿æŒå½“å‰å®ç°
        agent.high_level_agent.learning_rate = high_level_lr
        agent.high_level_agent.gamma = gamma_high
        agent.low_level_agent.learning_rate = low_level_lr
        agent.low_level_agent.gamma = gamma_low

        # è®­ç»ƒä¸€å°éƒ¨åˆ†æ­¥éª¤åè¯„ä¼°
        agent.train()
        # ä½¿ç”¨evaluate_detailedä»£æ›¿evaluate
        detailed_summary = agent.evaluate_detailed(eval_episodes=5)
        avg_reward = detailed_summary.get('avg_reward', 0.0)

        return avg_reward

    @staticmethod
    def optimize_hyperparameters():
        """ä½¿ç”¨Optunaä¼˜åŒ–è¶…å‚æ•°

        è¯¥æ–¹æ³•è´Ÿè´£åˆ›å»ºæˆ–åŠ è½½Optunaç ”ç©¶ï¼Œå¹¶æ‰§è¡Œè¶…å‚æ•°ä¼˜åŒ–è¿‡ç¨‹ã€‚è¶…å‚æ•°ä¼˜åŒ–æ˜¯
        å¼ºåŒ–å­¦ä¹ ä¸­çš„é‡è¦ç¯èŠ‚ï¼Œé€šè¿‡ç³»ç»Ÿåœ°æœç´¢è¶…å‚æ•°ç©ºé—´ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚
        è¯¥æ–¹æ³•ä¼šä¿å­˜æœ€ä¼˜å‚æ•°ä¾›åç»­è®­ç»ƒä½¿ç”¨ã€‚

        åŸç†:
            Optunaæ˜¯ä¸€ä¸ªè‡ªåŠ¨è¶…å‚æ•°ä¼˜åŒ–æ¡†æ¶ï¼Œå®ƒä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–ç®—æ³•é«˜æ•ˆåœ°æ¢ç´¢
            è¶…å‚æ•°ç©ºé—´ï¼Œæ‰¾åˆ°æœ€ä¼˜å‚æ•°ç»„åˆã€‚
        """
        print("å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–...")

        # åˆ›å»ºæˆ–åŠ è½½ç ”ç©¶
        try:
            study = optuna.load_study(study_name=OPTUNA_STUDY_NAME, storage="sqlite:///optuna.db")
            print("åŠ è½½å·²æœ‰ç ”ç©¶")
        except:
            study = optuna.create_study(
                study_name=OPTUNA_STUDY_NAME,
                storage="sqlite:///optuna.db",
                direction="maximize"
            )
            print("åˆ›å»ºæ–°ç ”ç©¶")

        # ä¼˜åŒ–ç›®æ ‡å‡½æ•°
        study.optimize(HierarchicalRL.objective, n_trials=OPTUNA_N_TRIALS, n_jobs=1)

        # æ‰“å°æœ€ä½³å‚æ•°
        print("æœ€ä½³å‚æ•°:")
        print(study.best_params)
        print(f"æœ€ä½³å¹³å‡å¥–åŠ±: {study.best_value:.2f}")

        # ä¿å­˜æœ€ä½³å‚æ•°
        os.makedirs("./configs", exist_ok=True)
        with open("./configs/best_hyperparams.txt", "w") as f:
            f.write(str(study.best_params))

        return study.best_params


    
    def evaluate_detailed(self, eval_episodes=10):
        """ä½¿ç”¨ç°æœ‰ç¯å¢ƒè¿›è¡Œè¯¦ç»†è¯„ä¼°ï¼ˆé¿å…ç¯å¢ƒå†²çªï¼‰
        
        è¯¥æ–¹æ³•ä½¿ç”¨å½“å‰è®­ç»ƒç¯å¢ƒè¿›è¡Œè¯¦ç»†è¯„ä¼°ï¼Œé¿å…å¯åŠ¨æ–°çš„Gazeboç¯å¢ƒã€‚
        åŒ…æ‹¬ï¼š
        - æˆåŠŸç‡ (success_rate)
        - è·¯å¾„æ•ˆç‡ (path_efficiency) 
        - è½¨è¿¹å¹³æ»‘åº¦ (trajectory_smoothness)
        - æ—¶é—´æˆæœ¬ (time_cost)
        - ç¢°æ’ç‡ (collision_rate)
        - å¹³å‡å¥–åŠ± (avg_reward) - æ–°å¢ç”¨äºä¸åŸæœ‰evaluateæ–¹æ³•å…¼å®¹
        
        å‚æ•°:
            eval_episodes: è¯„ä¼°çš„å›åˆæ•°
            
        è¿”å›:
            dict: è¯¦ç»†è¯„ä¼°ç»“æœ
        """
        print(f"å¼€å§‹è¯¦ç»†è¯„ä¼°ï¼Œå…± {eval_episodes} ä¸ªå›åˆ...")
        
        # ä½¿ç”¨ç°æœ‰ç¯å¢ƒè¿›è¡Œè¯„ä¼°ï¼Œé¿å…ç¯å¢ƒå†²çª
        episode_metrics = []
        success_episodes = 0
        collision_episodes = 0
        total_time_cost = 0.0
        total_path_efficiency = 0.0
        total_trajectory_smoothness = 0.0
        total_reward = 0.0  # æ–°å¢ï¼šç”¨äºè®¡ç®—å¹³å‡å¥–åŠ±
        
        for episode in range(eval_episodes):
            print(f"è¯„ä¼°å›åˆ {episode+1}/{eval_episodes}")
            
            # é‡ç½®ç¯å¢ƒ
            reset_result = self.env.reset()
            state = reset_result[0] if isinstance(reset_result, tuple) else reset_result
            if isinstance(state, tuple):
                state = state[0]
            state = np.array(state, dtype=np.float32)
            
            # åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡
            start_time = time.time()
            trajectory = []
            total_distance = 0.0
            episode_collision = False
            episode_reward = 0.0  # æ–°å¢ï¼šè®°å½•æœ¬å›åˆå¥–åŠ±
            
            # è·å–èµ·å§‹ä½ç½®å’Œç›®æ ‡ä½ç½®
            if hasattr(self.env, 'gazebo_env'):
                start_x = self.env.gazebo_env.odom_x
                start_y = self.env.gazebo_env.odom_y
                goal_x = self.env.gazebo_env.goal_x
                goal_y = self.env.gazebo_env.goal_y
                trajectory.append((start_x, start_y))
                
                straight_line_distance = np.linalg.norm([goal_x - start_x, goal_y - start_y])
            else:
                # å¦‚æœæ— æ³•è·å–ä½ç½®ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤å€¼
                start_x = start_y = goal_x = goal_y = 0.0
                trajectory.append((start_x, start_y))
                straight_line_distance = 1.0
            
            done = False
            episode_steps = 0
            self.prev_direction = 0.0
            
            while not done and episode_steps < self.max_ep:
                # é«˜å±‚å†³ç­– (ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥)
                high_level_action = self.high_level_agent.predict(state, deterministic=True)[0]
                direction = high_level_action % 20
                distance = (high_level_action // 20) * 0.5 + 0.5
                
                # ä½å±‚æ‰§è¡Œ (ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥)
                sub_goal_state = np.append(state, [direction, distance])
                low_level_action = self.low_level_agent.predict(sub_goal_state, deterministic=True)[0]
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, terminated, truncated, info = self.env.step(low_level_action)
                done = terminated or truncated
                
                # æ›´æ–°çŠ¶æ€
                if isinstance(next_state, tuple):
                    next_state = next_state[0]
                state = np.array(next_state, dtype=np.float32)
                
                # è®°å½•æœ¬å›åˆå¥–åŠ±
                episode_reward += reward
                
                # è®°å½•è½¨è¿¹
                if hasattr(self.env, 'gazebo_env'):
                    current_x = self.env.gazebo_env.odom_x
                    current_y = self.env.gazebo_env.odom_y
                    trajectory.append((current_x, current_y))
                    
                    # è®¡ç®—æ­¥é•¿
                    if len(trajectory) > 1:
                        prev_x, prev_y = trajectory[-2]
                        step_distance = np.linalg.norm([current_x - prev_x, current_y - prev_y])
                        total_distance += step_distance
                
                # æ£€æŸ¥ç¢°æ’
                if reward < -90 and not episode_collision:
                    episode_collision = True
                
                episode_steps += 1
            
            # ç´¯è®¡æ€»å¥–åŠ±
            total_reward += episode_reward
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            time_cost = time.time() - start_time
            total_time_cost += time_cost
            
            # è®¡ç®—æˆåŠŸç‡
            if hasattr(self.env, 'gazebo_env'):
                final_x = self.env.gazebo_env.odom_x
                final_y = self.env.gazebo_env.odom_y
                final_distance = np.linalg.norm([goal_x - final_x, goal_y - final_y])
                success = 1.0 if final_distance < 0.3 else 0.0
            else:
                success = 0.0
            
            if success >= 0.5:
                success_episodes += 1
            
            if episode_collision:
                collision_episodes += 1
            
            # è®¡ç®—è·¯å¾„æ•ˆç‡ï¼ˆåªæœ‰æˆåŠŸåˆ°è¾¾ç›®æ ‡æ—¶æ‰è®¡ç®—ï¼Œå¤±è´¥æ—¶è®¾ä¸º0ï¼‰
            if success >= 0.5:  # æˆåŠŸåˆ°è¾¾ç›®æ ‡
                if total_distance > 0.1:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç§»åŠ¨è·ç¦»
                    path_efficiency = min(1.0, straight_line_distance / total_distance)
                else:
                    path_efficiency = 1.0  # æˆåŠŸä½†ç§»åŠ¨è·ç¦»å¾ˆå°ï¼Œè®¤ä¸ºæ•ˆç‡å¾ˆé«˜
            else:  # æœªæˆåŠŸåˆ°è¾¾ç›®æ ‡
                path_efficiency = 0.0  # å¤±è´¥æ—¶è·¯å¾„æ•ˆç‡ä¸º0
            
            total_path_efficiency += path_efficiency
            
            # è®¡ç®—è½¨è¿¹å¹³æ»‘åº¦
            smoothness = self._calculate_trajectory_smoothness(trajectory)
            total_trajectory_smoothness += smoothness
            
            # ä¿å­˜å›åˆæŒ‡æ ‡ï¼ˆåŒ…å«è°ƒè¯•æ•°æ®ï¼‰
            episode_metrics.append({
                'success_rate': success,
                'time_cost': time_cost,
                'path_efficiency': path_efficiency,
                'trajectory_smoothness': smoothness,
                'reward': episode_reward,  # æ–°å¢ï¼šè®°å½•æœ¬å›åˆå¥–åŠ±
                # è°ƒè¯•æ•°æ®
                'debug_straight_line_distance': straight_line_distance,
                'debug_total_distance': total_distance,
                'debug_actual_distance': np.linalg.norm([final_x - start_x, final_y - start_y]) if hasattr(self.env, 'gazebo_env') else 0.0,
                'debug_start_pos': (start_x, start_y),
                'debug_final_pos': (final_x, final_y),
                'debug_goal_pos': (goal_x, goal_y),
                'debug_trajectory_length': len(trajectory),
                'debug_episode_steps': episode_steps
            })
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        n = len(episode_metrics) if episode_metrics else 1
        summary = {
            'episodes': n,
            'success_rate': success_episodes / n,
            'collision_rate': collision_episodes / n,
            'avg_time_cost': total_time_cost / n,
            'avg_path_efficiency': total_path_efficiency / n,
            'avg_trajectory_smoothness': total_trajectory_smoothness / n,
            'avg_reward': total_reward / n,  # æ–°å¢ï¼šå¹³å‡å¥–åŠ±
            # è°ƒè¯•æ•°æ®
            'avg_straight_line_distance': np.mean([m['debug_straight_line_distance'] for m in episode_metrics]),
            'avg_total_distance': np.mean([m['debug_total_distance'] for m in episode_metrics]),
            'avg_actual_distance': np.mean([m['debug_actual_distance'] for m in episode_metrics]),
            'avg_trajectory_length': np.mean([m['debug_trajectory_length'] for m in episode_metrics]),
            'avg_episode_steps': np.mean([m['debug_episode_steps'] for m in episode_metrics]),
        }
        
        # æ‰“å°è¯¦ç»†è¯„ä¼°ç»“æœ
        print(f"è¯¦ç»†è¯„ä¼°å®Œæˆ:")
        print(f"  æˆåŠŸç‡: {summary['success_rate']:.2%}")
        print(f"  ç¢°æ’ç‡: {summary['collision_rate']:.2%}")
        print(f"  å¹³å‡å¥–åŠ±: {summary['avg_reward']:.2f}")  # æ–°å¢ï¼šæ˜¾ç¤ºå¹³å‡å¥–åŠ±
        print(f"  å¹³å‡æ—¶é—´æˆæœ¬: {summary['avg_time_cost']:.2f}ç§’")
        print(f"  å¹³å‡è·¯å¾„æ•ˆç‡: {summary['avg_path_efficiency']:.2%}")
        print(f"  å¹³å‡è½¨è¿¹å¹³æ»‘åº¦: {summary['avg_trajectory_smoothness']:.2%}")
        print(f"ğŸ“Š è°ƒè¯•æ•°æ®:")
        print(f"  å¹³å‡ç›´çº¿è·ç¦»: {summary['avg_straight_line_distance']:.2f}m")
        print(f"  å¹³å‡æ€»è·ç¦»: {summary['avg_total_distance']:.2f}m")
        print(f"  å¹³å‡å®é™…è·ç¦»: {summary['avg_actual_distance']:.2f}m")
        print(f"  å¹³å‡è½¨è¿¹ç‚¹æ•°: {summary['avg_trajectory_length']:.1f}")
        print(f"  å¹³å‡å›åˆæ­¥æ•°: {summary['avg_episode_steps']:.1f}")
        print(f"ğŸ’¡ è¯´æ˜:")
        print(f"  - è·¯å¾„æ•ˆç‡: æˆåŠŸå›åˆçš„è·¯å¾„æ•ˆç‡ï¼Œå¤±è´¥å›åˆä¸º0%")
        print(f"  - æ•ˆç‡æ¯”å€¼: {summary['avg_straight_line_distance']:.2f} / {summary['avg_total_distance']:.2f} = {summary['avg_straight_line_distance']/summary['avg_total_distance']:.3f}")
        
        # ä¿å­˜è¯¦ç»†è¯„ä¼°ç»“æœï¼Œåªä¿å­˜è¿™ä¸€ä¸ªæ–‡ä»¶åˆ°logsæ–‡ä»¶å¤¹
        os.makedirs("./logs", exist_ok=True)
        detailed_evaluation_data = {
            'summary': summary,
            'episode_metrics': episode_metrics,
            'timestamp': time.time()
        }
        np.save("./logs/evaluation_metrics.npy", detailed_evaluation_data)
        
        # è®°å½•åˆ°TensorBoard
        self._log_evaluation_to_tensorboard(summary)
        
        return summary
    
    def _calculate_trajectory_smoothness(self, trajectory):
        """è®¡ç®—è½¨è¿¹å¹³æ»‘åº¦ï¼ˆåŸºäºè½¨è¿¹ç‚¹çš„æ›²ç‡å˜åŒ–ï¼‰"""
        if len(trajectory) < 3:
            return 1.0  # è½¨è¿¹ç‚¹å¤ªå°‘ï¼Œè®¤ä¸ºå¹³æ»‘
        
        # è®¡ç®—è½¨è¿¹çš„æ›²ç‡å˜åŒ–
        curvature_changes = []
        for i in range(1, len(trajectory) - 1):
            p1 = np.array(trajectory[i-1])
            p2 = np.array(trajectory[i])
            p3 = np.array(trajectory[i+1])
            
            # è®¡ç®—ä¸¤ä¸ªå‘é‡çš„å¤¹è§’
            v1 = p2 - p1
            v2 = p3 - p2
            
            if np.linalg.norm(v1) > 0 and np.linalg.norm(v2) > 0:
                cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
                cos_angle = np.clip(cos_angle, -1, 1)  # é˜²æ­¢æ•°å€¼è¯¯å·®
                angle = np.arccos(cos_angle)
                curvature_changes.append(angle)
        
        if not curvature_changes:
            return 1.0
        
        # å¹³æ»‘åº¦ = 1 / (1 + å¹³å‡æ›²ç‡å˜åŒ–)
        avg_curvature = np.mean(curvature_changes)
        smoothness = 1.0 / (1.0 + avg_curvature)
        return smoothness
    
    def _cleanup_environment(self):
        """æ¸…ç†ç¯å¢ƒï¼Œè§£å†³å†²çªé—®é¢˜"""
        try:
            print("æ­£åœ¨æ¸…ç†ç¯å¢ƒ...")
            
            # 1. å…³é—­å½“å‰ç¯å¢ƒ
            if hasattr(self, 'env') and hasattr(self.env, 'close'):
                self.env.close()
            
            # 2. æ¸…ç†ROSè¿›ç¨‹
            os.system("pkill -9 -f 'gazebo_ros/gzserver|gzserver|roslaunch|rosmaster|rosout|gzclient' 2>/dev/null || true")
            
            # 3. æ¸…ç†Gazeboå…±äº«å†…å­˜
            os.system("rm -f /dev/shm/gazebo-* /tmp/gazebo* 2>/dev/null || true")
            
            # 4. ç­‰å¾…è¿›ç¨‹å®Œå…¨ç»“æŸ
            import time
            time.sleep(2)
            
            print("ç¯å¢ƒæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            print(f"ç¯å¢ƒæ¸…ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    
    def _log_training_metrics_to_tensorboard(self, agent_type, timestep, episode_reward=None):
        """è®°å½•è®­ç»ƒæŒ‡æ ‡åˆ°TensorBoard"""
        try:
            if agent_type == "high_level":
                agent = self.high_level_agent
                prefix = "high_level"
            else:
                agent = self.low_level_agent
                prefix = "low_level"
            
            if hasattr(agent, '_logger') and agent._logger is not None:
                # è®°å½•å¥–åŠ±ï¼ˆå¦‚æœæä¾›ï¼‰
                if episode_reward is not None:
                    agent._logger.record(f"{prefix}/reward", episode_reward)
                
                # è®°å½•ç»éªŒç¼“å†²åŒºå¤§å°
                if hasattr(agent, 'replay_buffer'):
                    agent._logger.record(f"{prefix}/buffer_size", agent.replay_buffer.size())
                
                # è®°å½•å­¦ä¹ ç‡
                if hasattr(agent, 'learning_rate'):
                    agent._logger.record(f"{prefix}/learning_rate", agent.learning_rate)
                
                # è®°å½•æ¢ç´¢å‚æ•°
                if agent_type == "high_level" and hasattr(self, 'current_epsilon'):
                    agent._logger.record(f"{prefix}/epsilon", self.current_epsilon)
                elif agent_type == "low_level" and hasattr(self, 'current_noise'):
                    agent._logger.record(f"{prefix}/noise", self.current_noise)
                
                # è®°å½•æ›´æ–°æ¬¡æ•°
                if hasattr(agent, '_n_updates'):
                    agent._logger.record(f"{prefix}/n_updates", agent._n_updates)
                
                # è®°å½•åˆ°TensorBoard
                agent._logger.dump(step=timestep)
                
        except Exception as e:
            print(f"è®°å½•è®­ç»ƒæŒ‡æ ‡åˆ°TensorBoardæ—¶å‡ºé”™: {e}")
    
    def _log_evaluation_to_tensorboard(self, summary):
        """å°†è¯„ä¼°æŒ‡æ ‡è®°å½•åˆ°TensorBoard"""
        try:
            # ä¸ºé«˜å±‚å’Œä½å±‚æ™ºèƒ½ä½“éƒ½è®°å½•è¯„ä¼°æŒ‡æ ‡
            for agent_name, agent in [("high_level", self.high_level_agent), ("low_level", self.low_level_agent)]:
                if hasattr(agent, '_logger') and agent._logger is not None:
                    # è®°å½•è¯„ä¼°æŒ‡æ ‡
                    agent._logger.record("evaluation/success_rate", summary['success_rate'])
                    agent._logger.record("evaluation/collision_rate", summary['collision_rate'])
                    agent._logger.record("evaluation/avg_time_cost", summary['avg_time_cost'])
                    agent._logger.record("evaluation/avg_path_efficiency", summary['avg_path_efficiency'])
                    agent._logger.record("evaluation/avg_trajectory_smoothness", summary['avg_trajectory_smoothness'])
                    agent._logger.record("evaluation/episodes", summary['episodes'])
                    
                    # æ–°å¢ï¼šè®°å½•å¹³å‡å¥–åŠ±
                    if 'avg_reward' in summary:
                        agent._logger.record("evaluation/avg_reward", summary['avg_reward'])
                    
                    # è®°å½•è°ƒè¯•æ•°æ®
                    agent._logger.record("debug/avg_straight_line_distance", summary['avg_straight_line_distance'])
                    agent._logger.record("debug/avg_total_distance", summary['avg_total_distance'])
                    agent._logger.record("debug/avg_actual_distance", summary['avg_actual_distance'])
                    agent._logger.record("debug/avg_trajectory_length", summary['avg_trajectory_length'])
                    agent._logger.record("debug/avg_episode_steps", summary['avg_episode_steps'])
                    agent._logger.record("debug/efficiency_ratio", summary['avg_straight_line_distance']/summary['avg_total_distance'])
                    
                    # è®°å½•åˆ°TensorBoard
                    agent._logger.dump(step=agent.num_timesteps)
        except Exception as e:
            print(f"è®°å½•è¯„ä¼°æŒ‡æ ‡åˆ°TensorBoardæ—¶å‡ºé”™: {e}")
    
    def _cleanup_gazebo_ros(self):
        """
        è‡ªåŠ¨æ¸…ç†ï¼šå…³é—­ gym/envï¼Œæ€æ‰æ®‹ç•™çš„ ros/gazebo è¿›ç¨‹ï¼Œå¹¶æ¸…ç†å…±äº«å†…å­˜æ–‡ä»¶ã€‚
        åªåœ¨æœ¬è„šæœ¬å†…éƒ¨è°ƒç”¨ï¼Œä¸æ”¹å¤–éƒ¨å·¥ç¨‹ã€‚
        """
        # 1) å…ˆå°è¯•ä¼˜é›…å…³é—­ wrapper/envï¼ˆå¦‚æœå®ç°äº†ï¼‰
        try:
            if hasattr(self, "env") and hasattr(self.env, "close"):
                self.env.close()
        except Exception:
            pass
        # ä¹Ÿå°è¯•å…³æ‰ agent é‡Œå¯èƒ½æŒæœ‰çš„ env
        try:
            if hasattr(self, "high_level_agent") and hasattr(self.high_level_agent, "env") and self.high_level_agent.env is not None:
                self.high_level_agent.env.close()
        except Exception:
            pass
        try:
            if hasattr(self, "low_level_agent") and hasattr(self.low_level_agent, "env") and self.low_level_agent.env is not None:
                self.low_level_agent.env.close()
        except Exception:
            pass

        # 2) å…œåº•ï¼šå¹²æ‰å¸¸è§çš„ ros/gazebo è¿›ç¨‹ï¼ˆåªé’ˆå¯¹æœ¬æœºç”¨æˆ·ï¼Œ-f ä»¥å‘½ä»¤è¡ŒåŒ¹é…ï¼‰
        os.system("pkill -9 -f 'gazebo_ros/gzserver|gzserver -e ode|roslaunch|rosmaster|rosout|gzclient' 2>/dev/null || true")

        # 3) æ¸…ç† Gazebo å…±äº«å†…å­˜/é”æ–‡ä»¶ï¼Œé¿å…ä¸‹æ¬¡å¯åŠ¨ 255
        os.system("rm -f /dev/shm/gazebo-* /tmp/gazebo* 2>/dev/null || true")



def main(optimize=False):
    """ç¨‹åºä¸»å…¥å£ï¼Œæ”¯æŒè®­ç»ƒå’Œè¶…å‚æ•°ä¼˜åŒ–

    è¯¥å‡½æ•°æ˜¯ç¨‹åºçš„ä¸»å…¥å£ï¼Œè´Ÿè´£æ ¹æ®å‚æ•°å†³å®šæ‰§è¡Œè®­ç»ƒæˆ–è¶…å‚æ•°ä¼˜åŒ–ä»»åŠ¡ã€‚
    å¦‚æœå¯ç”¨è¶…å‚æ•°ä¼˜åŒ–ï¼Œå®ƒä¼šå…ˆè°ƒç”¨Optunaå¯»æ‰¾æœ€ä½³å‚æ•°ï¼Œç„¶åä½¿ç”¨è¿™äº›å‚æ•°
    åˆå§‹åŒ–æ¨¡å‹å¹¶å¼€å§‹è®­ç»ƒï¼›å¦åˆ™ï¼Œç›´æ¥ä½¿ç”¨é»˜è®¤å‚æ•°åˆå§‹åŒ–æ¨¡å‹å¹¶è®­ç»ƒã€‚

    å‚æ•°:
        optimize (bool): æ˜¯å¦è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–

    åŠŸèƒ½æµç¨‹:
        1. æ ¹æ®optimizeå‚æ•°å†³å®šæ˜¯å¦è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
        2. å¦‚æœè¿›è¡Œä¼˜åŒ–ï¼Œè°ƒç”¨HierarchicalRL.optimize_hyperparameters()è·å–æœ€ä½³å‚æ•°
        3. è¿‡æ»¤æ‰ä¸HierarchicalRLæ„é€ å‡½æ•°ä¸å…¼å®¹çš„å‚æ•°
        4. ä½¿ç”¨å‚æ•°åˆå§‹åŒ–HierarchicalRLå®ä¾‹
        5. è°ƒç”¨train()æ–¹æ³•å¼€å§‹è®­ç»ƒ
    """
    if optimize:
        # è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
        best_params = HierarchicalRL.optimize_hyperparameters()
        print("è¶…å‚æ•°ä¼˜åŒ–å®Œæˆï¼Œå¼€å§‹ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒ...")
        # è¿‡æ»¤æ‰ä¸HierarchicalRLæ„é€ å‡½æ•°ä¸å…¼å®¹çš„å‚æ•°
        valid_params = {k: v for k, v in best_params.items()
                        if k in HierarchicalRL.__init__.__code__.co_varnames}
        # ä½¿ç”¨æœ€ä½³å‚æ•°åˆ›å»ºå®ä¾‹å¹¶è®­ç»ƒ
        hierarchical_agent = HierarchicalRL(
            environment_dim=20,
            max_timesteps=2e6,
            **valid_params
        )
    else:
        # ç›´æ¥è®­ç»ƒ
        hierarchical_agent = HierarchicalRL(environment_dim=20, max_timesteps=2e6)

    # å¼€å§‹è®­ç»ƒ
    # â€”â€” å®‰è£…ä¿¡å·å¤„ç†ï¼šCtrl+C / kill éƒ½ä¼šå…ˆæ¸…ç†å†é€€å‡º
    def _sig_handler(sig, frame):
        try:
            hierarchical_agent._cleanup_gazebo_ros()
        finally:
            sys.exit(0)
    signal.signal(signal.SIGINT, _sig_handler)
    signal.signal(signal.SIGTERM, _sig_handler)

    # â€”â€” å¼€å§‹è®­ç»ƒï¼ˆç¡®ä¿æ— è®ºå¦‚ä½•éƒ½ä¼šæ¸…ç†ä¸€æ¬¡ï¼‰
    try:
        hierarchical_agent.train()
    finally:
        hierarchical_agent._cleanup_gazebo_ros()


if __name__ == "__main__":
    """ç¨‹åºå…¥å£ç‚¹

    å½“è„šæœ¬ç›´æ¥è¿è¡Œæ—¶ï¼Œè¿™éƒ¨åˆ†ä»£ç ä¼šè¢«æ‰§è¡Œã€‚å®ƒè´Ÿè´£è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œå¹¶è°ƒç”¨main
    å‡½æ•°å¼€å§‹æ‰§è¡Œç›¸åº”çš„ä»»åŠ¡ã€‚
    """
    import argparse

    # åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description="å±‚çº§å¼ºåŒ–å­¦ä¹ è®­ç»ƒ")
    # æ·»åŠ --optimizeå‚æ•°ï¼Œç”¨äºæ§åˆ¶æ˜¯å¦è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
    parser.add_argument("--optimize", action="store_true", help="æ˜¯å¦è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–")
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()

    # è°ƒç”¨mainå‡½æ•°ï¼Œä¼ å…¥optimizeå‚æ•°
    main(optimize=args.optimize)