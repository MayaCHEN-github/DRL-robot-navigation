import numpy as np
import time
from gym_wrapper import VelodyneGymWrapper
from velodyne_env import GazeboEnv

class EvaluationWrapper(VelodyneGymWrapper):
    """
    ç»§æ‰¿VelodyneGymWrapperï¼Œæ·»åŠ è¯„ä¼°åŠŸèƒ½
    ç”¨äºè®¡ç®—å¯¼èˆªæ•ˆç‡å’Œæ€§èƒ½æŒ‡æ ‡
    """
    
    def __init__(self, launchfile, environment_dim, action_type="continuous"):
        super().__init__(launchfile, environment_dim, action_type)
        self.reset_evaluation_metrics() # åˆå§‹åŒ–metrics
    
    def step(self, action):
        state, reward, done, info = super().step(action)
        self._update_evaluation_metrics() # æ›´æ–°metrics
        
        return state, reward, done, info
    
    def reset(self):
        state = super().reset() # è°ƒç”¨çˆ¶ç±»çš„resetæ–¹æ³•
        self.reset_evaluation_metrics() # é‡ç½®metrics
        self._initialize_evaluation_metrics() # è·å–èµ·å§‹ä½ç½®å’Œç›®æ ‡ä½ç½®
        return state
    
    def _initialize_evaluation_metrics(self):
        """åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡ï¼Œç§æœ‰æ–¹æ³•"""
        # è·å–èµ·å§‹ä½ç½®ï¼ˆæœºå™¨äººå½“å‰ä½ç½®ï¼‰
        self.start_x = self.gazebo_env.odom_x
        self.start_y = self.gazebo_env.odom_y
        self.current_x = self.start_x
        self.current_y = self.start_y
        # è·å–ç›®æ ‡ä½ç½®
        self.goal_x = self.gazebo_env.goal_x
        self.goal_y = self.gazebo_env.goal_y
        # è®¡ç®—ç›´çº¿è·ç¦»
        self.straight_line_distance = np.linalg.norm([
            self.goal_x - self.start_x,
            self.goal_y - self.start_y
        ])
        # åˆå§‹åŒ–è½¨è¿¹
        self.trajectory = [(self.start_x, self.start_y)]
        
        # åˆå§‹åŒ–æ–°å¢æŒ‡æ ‡
        self.start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
        self.collision_count = 0
        self.total_steps = 0
        self.path_changes = 0
        self.last_direction = None
    
    def _update_evaluation_metrics(self):
        """æ›´æ–°è¯„ä¼°æŒ‡æ ‡ï¼Œç§æœ‰æ–¹æ³•"""
        # è·å–å½“å‰ä½ç½®
        self.current_x = self.gazebo_env.odom_x
        self.current_y = self.gazebo_env.odom_y
        # æ·»åŠ åˆ°è½¨è¿¹
        self.trajectory.append((self.current_x, self.current_y))
        # è®¡ç®—è¿™ä¸€æ­¥ç§»åŠ¨çš„è·ç¦»ï¼ˆç´¯è®¡æ€»è·¯ç¨‹ï¼Œæ¬§å‡ é‡Œå¾—è·ç¦»ï¼‰
        if len(self.trajectory) > 1:
            prev_x, prev_y = self.trajectory[-2]
            step_distance = np.linalg.norm([
                self.current_x - prev_x,
                self.current_y - prev_y
            ])
            self.total_distance_traveled += step_distance
            
            # æ£€æµ‹è·¯å¾„å˜åŒ–ï¼ˆé‡è§„åˆ’é¢‘ç‡ï¼‰
            current_direction = np.arctan2(self.current_y - prev_y, self.current_x - prev_x)
            if self.last_direction is not None:
                direction_change = abs(current_direction - self.last_direction)
                if direction_change > np.pi/4:  # 45åº¦ä»¥ä¸Šçš„æ–¹å‘å˜åŒ–
                    self.path_changes += 1
            self.last_direction = current_direction
        
        # æ›´æ–°æ­¥æ•°
        self.total_steps += 1
        
        # æ£€æµ‹ç¢°æ’ï¼ˆé€šè¿‡æ¿€å…‰é›·è¾¾æ•°æ®ï¼‰
        if hasattr(self.gazebo_env, 'velodyne_data'):
            min_laser = min(self.gazebo_env.velodyne_data)
            if min_laser < 0.35:  # ç¢°æ’é˜ˆå€¼
                self.collision_count += 1

    def reset_evaluation_metrics(self):
        """é‡ç½®è¯„ä¼°æŒ‡æ ‡ï¼Œå…¬æœ‰æ–¹æ³•ï¼ˆå› ä¸ºå¯èƒ½ä¼šåœ¨å¤–éƒ¨è°ƒç”¨ï¼Ÿï¼‰"""
        # æœºå™¨äººèµ·å§‹ä½ç½®
        self.start_x = None
        self.start_y = None
        # ç›®æ ‡ä½ç½®
        self.goal_x = None
        self.goal_y = None
        # æœºå™¨äººå½“å‰ä½ç½®
        self.current_x = None
        self.current_y = None
        # è¿åŠ¨è½¨è¿¹
        self.trajectory = []
        # æ€»è·¯ç¨‹
        self.total_distance_traveled = 0.0
        # ç›´çº¿è·ç¦»ï¼ˆèµ·å§‹ç‚¹åˆ°ç›®æ ‡ç‚¹ï¼‰
        self.straight_line_distance = 0.0
        
        # æ–°å¢è¯„ä¼°æŒ‡æ ‡
        self.start_time = None  # å¼€å§‹æ—¶é—´
        self.collision_count = 0  # ç¢°æ’æ¬¡æ•°
        self.total_steps = 0  # æ€»æ­¥æ•°
        self.path_changes = 0  # è·¯å¾„å˜åŒ–æ¬¡æ•°
        self.last_direction = None  # ä¸Šä¸€æ­¥çš„è¿åŠ¨æ–¹å‘
    
    def get_evaluation_metrics(self):
        """è·å–è¯„ä¼°æŒ‡æ ‡"""
        # å®‰å…¨æ£€æŸ¥
        if self.start_x is None or self.goal_x is None or self.current_x is None:
            print("Warning: Evaluation metrics not initialized yet")
            return None
        # è®¡ç®—å½“å‰åˆ°ç›®æ ‡çš„è·ç¦»
        current_to_goal_distance = np.linalg.norm([
            self.goal_x - self.current_x,
            self.goal_y - self.current_y
        ])
        # è®¡ç®—æ•ˆç‡æ¯”å€¼ï¼ˆç›´çº¿è·ç¦» / æ€»è·¯ç¨‹ï¼‰
        efficiency_ratio = 0.0
        if self.total_distance_traveled > 0:
            efficiency_ratio = self.straight_line_distance / self.total_distance_traveled
        else:
            efficiency_ratio = 1.0  # å¦‚æœè¿˜æ²¡ç§»åŠ¨ï¼Œæ•ˆç‡ä¸º100%ï¼ˆè™½ç„¶ä¸ä¸€å®šæœ‰æ„ä¹‰â€¦â€¦ï¼‰
        
        # è®¡ç®—æ–°å¢æŒ‡æ ‡
        time_cost = time.time() - self.start_time if self.start_time else 0
        success_rate = 1.0 if current_to_goal_distance < 0.3 else 0.0
        collision_rate = self.collision_count / max(self.total_steps, 1)
        
        # è®¡ç®—è½¨è¿¹å¹³æ»‘åº¦ï¼ˆé€šè¿‡è½¨è¿¹ç‚¹çš„æ›²ç‡å˜åŒ–ï¼‰
        smoothness = self._calculate_trajectory_smoothness()
        
        # è®¡ç®—å ç”¨ç‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼ŒåŸºäºæ¿€å…‰é›·è¾¾æ•°æ®ï¼‰
        occupancy = self._calculate_occupancy_percentage()
        
        return {
            'start_position': (self.start_x, self.start_y),
            'goal_position': (self.goal_x, self.goal_y),
            'current_position': (self.current_x, self.current_y),
            'straight_line_distance': self.straight_line_distance,
            'total_distance_traveled': self.total_distance_traveled,
            'current_to_goal_distance': current_to_goal_distance,
            'efficiency_ratio': efficiency_ratio,
            'trajectory_length': len(self.trajectory),
            
            # æ–°å¢çš„7ä¸ªè¯„ä¼°æŒ‡æ ‡
            'success_rate': success_rate,                    # 1. æˆåŠŸç‡
            'occupancy_percentage': occupancy,               # 2. å ç”¨ç‡
            'collision_rate': collision_rate,                # 3. ç¢°æ’ç‡
            'time_cost': time_cost,                          # 4. æ—¶é—´æˆæœ¬
            'path_efficiency': efficiency_ratio,             # 5. è·¯å¾„æ•ˆç‡
            'trajectory_smoothness': smoothness,             # 6. è½¨è¿¹å¹³æ»‘åº¦
            'replanning_frequency': self.path_changes        # 7. é‡è§„åˆ’é¢‘ç‡
        }
    
    def print_evaluation_summary(self):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        metrics = self.get_evaluation_metrics()
        
        print("=" * 50)
        print("EVALUATION SUMMARY è¯„ä¼°æ‘˜è¦")
        print("=" * 50)
        print(f"ğŸ“ èµ·å§‹ä½ç½®: ({metrics['start_position'][0]:.2f}, {metrics['start_position'][1]:.2f})")
        print(f"ğŸ¯ ç›®æ ‡ä½ç½®:  ({metrics['goal_position'][0]:.2f}, {metrics['goal_position'][1]:.2f})")
        print(f"ğŸ¤– å½“å‰ä½ç½®: ({metrics['current_position'][0]:.2f}, {metrics['current_position'][1]:.2f})")
        print("-" * 50)
        print(f"ğŸ“ ç›´çº¿è·ç¦»: {metrics['straight_line_distance']:.2f} m")
        print(f"ğŸ›¤ï¸ æ€»è·¯ç¨‹: {metrics['total_distance_traveled']:.2f} m")
        print(f"ğŸ“ å½“å‰åˆ°ç›®æ ‡è·ç¦»: {metrics['current_to_goal_distance']:.2f} m")
        print(f"ğŸ’¡ æ•ˆç‡æ¯”å€¼: {metrics['efficiency_ratio']:.3f}")
        print(f"ğŸ“Š è½¨è¿¹ç‚¹æ•°: {metrics['trajectory_length']}")
        print("-" * 50)
        print("ğŸ“Š æ–°å¢è¯„ä¼°æŒ‡æ ‡:")
        print(f"ğŸ¯ æˆåŠŸç‡: {metrics['success_rate']:.1%}")
        print(f"ğŸ—ï¸ å ç”¨ç‡: {metrics['occupancy_percentage']:.1%}")
        print(f"ğŸ’¥ ç¢°æ’ç‡: {metrics['collision_rate']:.1%}")
        print(f"â±ï¸ æ—¶é—´æˆæœ¬: {metrics['time_cost']:.2f}s")
        print(f"ğŸ“ˆ è·¯å¾„æ•ˆç‡: {metrics['path_efficiency']:.3f}")
        print(f"ğŸ”„ è½¨è¿¹å¹³æ»‘åº¦: {metrics['trajectory_smoothness']:.3f}")
        print(f"ğŸ”„ é‡è§„åˆ’é¢‘ç‡: {metrics['replanning_frequency']}")
        print("=" * 50)
    
    def _calculate_trajectory_smoothness(self):
        """è®¡ç®—è½¨è¿¹å¹³æ»‘åº¦ï¼ˆåŸºäºè½¨è¿¹ç‚¹çš„æ›²ç‡å˜åŒ–ï¼‰"""
        if len(self.trajectory) < 3:
            return 1.0  # è½¨è¿¹ç‚¹å¤ªå°‘ï¼Œè®¤ä¸ºå¹³æ»‘
        
        # è®¡ç®—è½¨è¿¹çš„æ›²ç‡å˜åŒ–
        curvature_changes = []
        for i in range(1, len(self.trajectory) - 1):
            p1 = np.array(self.trajectory[i-1])
            p2 = np.array(self.trajectory[i])
            p3 = np.array(self.trajectory[i+1])
            
            # è®¡ç®—ä¸¤ä¸ªå‘é‡çš„å¤¹è§’
            v1 = p2 - p1
            v2 = p3 - p2
            
            if np.linalg.norm(v1) > 0 and np.linalg.norm(v2) > 0:
                cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
                cos_angle = np.clip(cos_angle, -1, 1)  # é˜²æ­¢æ•°å€¼è¯¯å·®
                angle = np.arccos(cos_angle)
                curvature_changes.append(angle)
        
        if not curvature_changes:
            return 1.0
        
        # å¹³æ»‘åº¦ = 1 / (1 + å¹³å‡æ›²ç‡å˜åŒ–)
        avg_curvature = np.mean(curvature_changes)
        smoothness = 1.0 / (1.0 + avg_curvature)
        return smoothness
    
    def _calculate_occupancy_percentage(self):
        """è®¡ç®—ç¯å¢ƒå ç”¨ç‡ï¼ˆåŸºäºæ¿€å…‰é›·è¾¾æ•°æ®ï¼‰"""
        if not hasattr(self.gazebo_env, 'velodyne_data'):
            return 0.0
        
        # ç»Ÿè®¡æ¿€å…‰é›·è¾¾æ•°æ®ä¸­è·ç¦»å°äºé˜ˆå€¼çš„ç‚¹æ•°
        laser_data = self.gazebo_env.velodyne_data
        if len(laser_data) == 0:
            return 0.0
        
        # è·ç¦»å°äº2ç±³çš„ç‚¹è®¤ä¸ºæ˜¯éšœç¢ç‰©
        obstacle_threshold = 2.0
        obstacle_count = np.sum(laser_data < obstacle_threshold)
        total_count = len(laser_data)
        
        occupancy = obstacle_count / total_count
        return occupancy

class GazeboEvaluationWrapper:
    """ä¸ºGazeboEnvæ·»åŠ è¯„ä¼°åŠŸèƒ½çš„åŒ…è£…å™¨"""
    
    def __init__(self, gazebo_env):
        self.gazebo_env = gazebo_env
        self.reset_evaluation_metrics()
    
    def step(self, action):
        state, reward, done, info = self.gazebo_env.step(action)
        self._update_evaluation_metrics()
        return state, reward, done, info
    
    def reset(self):
        state = self.gazebo_env.reset()
        self.reset_evaluation_metrics()
        self._initialize_evaluation_metrics()
        return state
    
    def _initialize_evaluation_metrics(self):
        """åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡"""
        self.start_x = self.gazebo_env.odom_x
        self.start_y = self.gazebo_env.odom_y
        self.current_x = self.start_x
        self.current_y = self.start_y
        self.goal_x = self.gazebo_env.goal_x
        self.goal_y = self.gazebo_env.goal_y
        self.straight_line_distance = np.linalg.norm([
            self.goal_x - self.start_x,
            self.goal_y - self.start_y
        ])
        self.trajectory = [(self.start_x, self.start_y)]
    
    def _update_evaluation_metrics(self):
        """æ›´æ–°è¯„ä¼°æŒ‡æ ‡"""
        self.current_x = self.gazebo_env.odom_x
        self.current_y = self.gazebo_env.odom_y
        self.trajectory.append((self.current_x, self.current_y))
        
        if len(self.trajectory) > 1:
            prev_x, prev_y = self.trajectory[-2]
            step_distance = np.linalg.norm([
                self.current_x - prev_x,
                self.current_y - prev_y
            ])
            self.total_distance_traveled += step_distance
    
    def reset_evaluation_metrics(self):
        """é‡ç½®è¯„ä¼°æŒ‡æ ‡"""
        self.start_x = None
        self.start_y = None
        self.goal_x = None
        self.goal_y = None
        self.current_x = None
        self.current_y = None
        self.trajectory = []
        self.total_distance_traveled = 0.0
        self.straight_line_distance = 0.0
    
    def get_evaluation_metrics(self):
        """è·å–è¯„ä¼°æŒ‡æ ‡"""
        if self.start_x is None or self.goal_x is None or self.current_x is None:
            print("Warning: Evaluation metrics not initialized yet")
            return None
        
        current_to_goal_distance = np.linalg.norm([
            self.goal_x - self.current_x,
            self.goal_y - self.current_y
        ])
        
        efficiency_ratio = 0.0
        if self.total_distance_traveled > 0:
            efficiency_ratio = self.straight_line_distance / self.total_distance_traveled
        else:
            efficiency_ratio = 1.0
        
        return {
            'start_position': (self.start_x, self.start_y),
            'goal_position': (self.goal_x, self.goal_y),
            'current_position': (self.current_x, self.current_y),
            'straight_line_distance': self.straight_line_distance,
            'total_distance_traveled': self.total_distance_traveled,
            'current_to_goal_distance': current_to_goal_distance,
            'efficiency_ratio': efficiency_ratio,
            'trajectory_length': len(self.trajectory)
        }
    
    def print_evaluation_summary(self):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        metrics = self.get_evaluation_metrics()
        
        print("=" * 50)
        print("EVALUATION SUMMARY è¯„ä¼°æ‘˜è¦")
        print("=" * 50)
        print(f"ğŸ“ èµ·å§‹ä½ç½®: ({metrics['start_position'][0]:.2f}, {metrics['start_position'][1]:.2f})")
        print(f"ğŸ¯ ç›®æ ‡ä½ç½®:  ({metrics['goal_position'][0]:.2f}, {metrics['goal_position'][1]:.2f})")
        print(f"ğŸ¤– å½“å‰ä½ç½®: ({metrics['current_position'][0]:.2f}, {metrics['current_position'][1]:.2f})")
        print("-" * 50)
        print(f"ğŸ“ ç›´çº¿è·ç¦»: {metrics['straight_line_distance']:.2f} m")
        print(f"ğŸ›¤ï¸ æ€»è·¯ç¨‹: {metrics['total_distance_traveled']:.2f} m")
        print(f"ğŸ“ å½“å‰åˆ°ç›®æ ‡è·ç¦»: {metrics['current_to_goal_distance']:.2f} m")
        print(f"ğŸ’¡ æ•ˆç‡æ¯”å€¼: {metrics['efficiency_ratio']:.3f}")
        print(f"ğŸ“Š è½¨è¿¹ç‚¹æ•°: {metrics['trajectory_length']}")

# ç»Ÿä¸€çš„è¯„ä¼°å‡½æ•°
def evaluate_with_metrics(network, env, epoch, eval_episodes=10, use_evaluation=True):
    """
    ç»Ÿä¸€çš„è¯„ä¼°å‡½æ•°ï¼Œæ”¯æŒä¸åŒç±»å‹çš„ç¯å¢ƒå’Œç½‘ç»œ
    
    Args:
        network: è®­ç»ƒå¥½çš„ç½‘ç»œï¼ˆéœ€è¦æœ‰get_actionæ–¹æ³•ï¼‰
        env: ç¯å¢ƒå®ä¾‹ï¼ˆGazeboEnvæˆ–VelodyneGymWrapperï¼‰
        epoch: å½“å‰è®­ç»ƒè½®æ¬¡
        eval_episodes: è¯„ä¼°å›åˆæ•°
        use_evaluation: æ˜¯å¦ä½¿ç”¨è¯„ä¼°æŒ‡æ ‡
    """
    if use_evaluation:
        # æ ¹æ®ç¯å¢ƒç±»å‹é€‰æ‹©åˆé€‚çš„è¯„ä¼°åŒ…è£…å™¨
        if isinstance(env, GazeboEnv):
            eval_env = GazeboEvaluationWrapper(env)
        elif hasattr(env, 'gazebo_env'):  # VelodyneGymWrapper
            eval_env = EvaluationWrapper(env.launchfile, env.environment_dim, env.action_type)
        else:
            eval_env = None
            print("Warning: Unknown environment type, evaluation metrics disabled")
    else:
        eval_env = None
    
    avg_reward = 0.0
    col = 0
    
    for _ in range(eval_episodes):
        count = 0
        state = env.reset()
        if eval_env:
            eval_env.reset()
        
        done = False
        while not done and count < 501:
            action = network.get_action(np.array(state))
            
            # å¤„ç†ä¸åŒç®—æ³•çš„åŠ¨ä½œæ ¼å¼
            if hasattr(network, 'num_actions'):  # DQNç®—æ³•
                a_in = [(action[0] + 1) / 2, action[1]]
            else:  # TD3ç­‰è¿ç»­åŠ¨ä½œç®—æ³•
                a_in = action
            
            state, reward, done, _ = env.step(a_in)
            if eval_env:
                eval_env.step(a_in)
            
            avg_reward += reward
            count += 1
            if reward < -90:
                col += 1
    
    avg_reward /= eval_episodes
    avg_col = col / eval_episodes
    
    print("..............................................")
    print(
        "Average Reward over %i Evaluation Episodes, Epoch %i: %f, %f"
        % (eval_episodes, epoch, avg_reward, avg_col)
    )
    print("..............................................")
    
    # æ‰“å°è¯„ä¼°æŒ‡æ ‡
    if eval_env:
        eval_env.print_evaluation_summary()
    
    return avg_reward


