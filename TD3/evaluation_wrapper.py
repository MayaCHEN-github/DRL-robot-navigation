import numpy as np
from gym_wrapper import VelodyneGymWrapper
from velodyne_env import GazeboEnv

class EvaluationWrapper(VelodyneGymWrapper):
    """
    ç»§æ‰¿VelodyneGymWrapperï¼Œæ·»åŠ è¯„ä¼°åŠŸèƒ½
    ç”¨äºè®¡ç®—å¯¼èˆªæ•ˆç‡å’Œæ€§èƒ½æŒ‡æ ‡
    """
    
    def __init__(self, launchfile, environment_dim, action_type="continuous"):
        super().__init__(launchfile, environment_dim, action_type)
        self.reset_evaluation_metrics() # åˆå§‹åŒ–metrics
    
    def step(self, action):
        state, reward, done, info = super().step(action)
        self._update_evaluation_metrics() # æ›´æ–°metrics
        
        return state, reward, done, info
    
    def reset(self):
        state = super().reset() # è°ƒç”¨çˆ¶ç±»çš„resetæ–¹æ³•
        self.reset_evaluation_metrics() # é‡ç½®metrics
        self._initialize_evaluation_metrics() # è·å–èµ·å§‹ä½ç½®å’Œç›®æ ‡ä½ç½®
        return state
    
    def _initialize_evaluation_metrics(self):
        """åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡ï¼Œç§æœ‰æ–¹æ³•"""
        # è·å–èµ·å§‹ä½ç½®ï¼ˆæœºå™¨äººå½“å‰ä½ç½®ï¼‰
        self.start_x = self.gazebo_env.odom_x
        self.start_y = self.gazebo_env.odom_y
        self.current_x = self.start_x
        self.current_y = self.start_y
        # è·å–ç›®æ ‡ä½ç½®
        self.goal_x = self.gazebo_env.goal_x
        self.goal_y = self.gazebo_env.goal_y
        # è®¡ç®—ç›´çº¿è·ç¦»
        self.straight_line_distance = np.linalg.norm([
            self.goal_x - self.start_x,
            self.goal_y - self.start_y
        ])
        # åˆå§‹åŒ–è½¨è¿¹
        self.trajectory = [(self.start_x, self.start_y)]
    
    def _update_evaluation_metrics(self):
        """æ›´æ–°è¯„ä¼°æŒ‡æ ‡ï¼Œç§æœ‰æ–¹æ³•"""
        # è·å–å½“å‰ä½ç½®
        self.current_x = self.gazebo_env.odom_x
        self.current_y = self.gazebo_env.odom_y
        # æ·»åŠ åˆ°è½¨è¿¹
        self.trajectory.append((self.current_x, self.current_y))
        # è®¡ç®—è¿™ä¸€æ­¥ç§»åŠ¨çš„è·ç¦»ï¼ˆç´¯è®¡æ€»è·¯ç¨‹ï¼Œæ¬§å‡ é‡Œå¾—è·ç¦»ï¼‰
        if len(self.trajectory) > 1:
            prev_x, prev_y = self.trajectory[-2]
            step_distance = np.linalg.norm([
                self.current_x - prev_x,
                self.current_y - prev_y
            ])
            self.total_distance_traveled += step_distance

    def reset_evaluation_metrics(self):
        """é‡ç½®è¯„ä¼°æŒ‡æ ‡ï¼Œå…¬æœ‰æ–¹æ³•ï¼ˆå› ä¸ºå¯èƒ½ä¼šåœ¨å¤–éƒ¨è°ƒç”¨ï¼Ÿï¼‰"""
        # æœºå™¨äººèµ·å§‹ä½ç½®
        self.start_x = None
        self.start_y = None
        # ç›®æ ‡ä½ç½®
        self.goal_x = None
        self.goal_y = None
        # æœºå™¨äººå½“å‰ä½ç½®
        self.current_x = None
        self.current_y = None
        # è¿åŠ¨è½¨è¿¹
        self.trajectory = []
        # æ€»è·¯ç¨‹
        self.total_distance_traveled = 0.0
        # ç›´çº¿è·ç¦»ï¼ˆèµ·å§‹ç‚¹åˆ°ç›®æ ‡ç‚¹ï¼‰
        self.straight_line_distance = 0.0
    
    def get_evaluation_metrics(self):
        """è·å–è¯„ä¼°æŒ‡æ ‡"""
        # å®‰å…¨æ£€æŸ¥
        if self.start_x is None or self.goal_x is None or self.current_x is None:
            print("Warning: Evaluation metrics not initialized yet")
            return None
        # è®¡ç®—å½“å‰åˆ°ç›®æ ‡çš„è·ç¦»
        current_to_goal_distance = np.linalg.norm([
            self.goal_x - self.current_x,
            self.goal_y - self.current_y
        ])
        # è®¡ç®—æ•ˆç‡æ¯”å€¼ï¼ˆç›´çº¿è·ç¦» / æ€»è·¯ç¨‹ï¼‰
        efficiency_ratio = 0.0
        if self.total_distance_traveled > 0:
            efficiency_ratio = self.straight_line_distance / self.total_distance_traveled
        else:
            efficiency_ratio = 1.0  # å¦‚æœè¿˜æ²¡ç§»åŠ¨ï¼Œæ•ˆç‡ä¸º100%ï¼ˆè™½ç„¶ä¸ä¸€å®šæœ‰æ„ä¹‰â€¦â€¦ï¼‰
        
        return {
            'start_position': (self.start_x, self.start_y),
            'goal_position': (self.goal_x, self.goal_y),
            'current_position': (self.current_x, self.current_y),
            'straight_line_distance': self.straight_line_distance,
            'total_distance_traveled': self.total_distance_traveled,
            'current_to_goal_distance': current_to_goal_distance,
            'efficiency_ratio': efficiency_ratio,
            'trajectory_length': len(self.trajectory)
        }
    
    def print_evaluation_summary(self):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        metrics = self.get_evaluation_metrics()
        
        print("=" * 50)
        print("EVALUATION SUMMARY è¯„ä¼°æ‘˜è¦")
        print("=" * 50)
        print(f"ğŸ“ èµ·å§‹ä½ç½®: ({metrics['start_position'][0]:.2f}, {metrics['start_position'][1]:.2f})")
        print(f"ğŸ¯ ç›®æ ‡ä½ç½®:  ({metrics['goal_position'][0]:.2f}, {metrics['goal_position'][1]:.2f})")
        print(f"ğŸ¤– å½“å‰ä½ç½®: ({metrics['current_position'][0]:.2f}, {metrics['current_position'][1]:.2f})")
        print("-" * 50)
        print(f"ğŸ“ ç›´çº¿è·ç¦»: {metrics['straight_line_distance']:.2f} m")
        print(f"ğŸ›¤ï¸ æ€»è·¯ç¨‹: {metrics['total_distance_traveled']:.2f} m")
        print(f"ğŸ“ å½“å‰åˆ°ç›®æ ‡è·ç¦»: {metrics['current_to_goal_distance']:.2f} m")
        print(f"ğŸ’¡ æ•ˆç‡æ¯”å€¼: {metrics['efficiency_ratio']:.3f}")
        print(f"ğŸ“Š è½¨è¿¹ç‚¹æ•°: {metrics['trajectory_length']}")
        print("=" * 50)

class GazeboEvaluationWrapper:
    """ä¸ºGazeboEnvæ·»åŠ è¯„ä¼°åŠŸèƒ½çš„åŒ…è£…å™¨"""
    
    def __init__(self, gazebo_env):
        self.gazebo_env = gazebo_env
        self.reset_evaluation_metrics()
    
    def step(self, action):
        state, reward, done, info = self.gazebo_env.step(action)
        self._update_evaluation_metrics()
        return state, reward, done, info
    
    def reset(self):
        state = self.gazebo_env.reset()
        self.reset_evaluation_metrics()
        self._initialize_evaluation_metrics()
        return state
    
    def _initialize_evaluation_metrics(self):
        """åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡"""
        self.start_x = self.gazebo_env.odom_x
        self.start_y = self.gazebo_env.odom_y
        self.current_x = self.start_x
        self.current_y = self.start_y
        self.goal_x = self.gazebo_env.goal_x
        self.goal_y = self.gazebo_env.goal_y
        self.straight_line_distance = np.linalg.norm([
            self.goal_x - self.start_x,
            self.goal_y - self.start_y
        ])
        self.trajectory = [(self.start_x, self.start_y)]
    
    def _update_evaluation_metrics(self):
        """æ›´æ–°è¯„ä¼°æŒ‡æ ‡"""
        self.current_x = self.gazebo_env.odom_x
        self.current_y = self.gazebo_env.odom_y
        self.trajectory.append((self.current_x, self.current_y))
        
        if len(self.trajectory) > 1:
            prev_x, prev_y = self.trajectory[-2]
            step_distance = np.linalg.norm([
                self.current_x - prev_x,
                self.current_y - prev_y
            ])
            self.total_distance_traveled += step_distance
    
    def reset_evaluation_metrics(self):
        """é‡ç½®è¯„ä¼°æŒ‡æ ‡"""
        self.start_x = None
        self.start_y = None
        self.goal_x = None
        self.goal_y = None
        self.current_x = None
        self.current_y = None
        self.trajectory = []
        self.total_distance_traveled = 0.0
        self.straight_line_distance = 0.0
    
    def get_evaluation_metrics(self):
        """è·å–è¯„ä¼°æŒ‡æ ‡"""
        if self.start_x is None or self.goal_x is None or self.current_x is None:
            print("Warning: Evaluation metrics not initialized yet")
            return None
        
        current_to_goal_distance = np.linalg.norm([
            self.goal_x - self.current_x,
            self.goal_y - self.current_y
        ])
        
        efficiency_ratio = 0.0
        if self.total_distance_traveled > 0:
            efficiency_ratio = self.straight_line_distance / self.total_distance_traveled
        else:
            efficiency_ratio = 1.0
        
        return {
            'start_position': (self.start_x, self.start_y),
            'goal_position': (self.goal_x, self.goal_y),
            'current_position': (self.current_x, self.current_y),
            'straight_line_distance': self.straight_line_distance,
            'total_distance_traveled': self.total_distance_traveled,
            'current_to_goal_distance': current_to_goal_distance,
            'efficiency_ratio': efficiency_ratio,
            'trajectory_length': len(self.trajectory)
        }
    
    def print_evaluation_summary(self):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        metrics = self.get_evaluation_metrics()
        
        print("=" * 50)
        print("EVALUATION SUMMARY è¯„ä¼°æ‘˜è¦")
        print("=" * 50)
        print(f"ğŸ“ èµ·å§‹ä½ç½®: ({metrics['start_position'][0]:.2f}, {metrics['start_position'][1]:.2f})")
        print(f"ğŸ¯ ç›®æ ‡ä½ç½®:  ({metrics['goal_position'][0]:.2f}, {metrics['goal_position'][1]:.2f})")
        print(f"ğŸ¤– å½“å‰ä½ç½®: ({metrics['current_position'][0]:.2f}, {metrics['current_position'][1]:.2f})")
        print("-" * 50)
        print(f"ğŸ“ ç›´çº¿è·ç¦»: {metrics['straight_line_distance']:.2f} m")
        print(f"ğŸ›¤ï¸ æ€»è·¯ç¨‹: {metrics['total_distance_traveled']:.2f} m")
        print(f"ğŸ“ å½“å‰åˆ°ç›®æ ‡è·ç¦»: {metrics['current_to_goal_distance']:.2f} m")
        print(f"ğŸ’¡ æ•ˆç‡æ¯”å€¼: {metrics['efficiency_ratio']:.3f}")
        print(f"ğŸ“Š è½¨è¿¹ç‚¹æ•°: {metrics['trajectory_length']}")

# ç»Ÿä¸€çš„è¯„ä¼°å‡½æ•°
def evaluate_with_metrics(network, env, epoch, eval_episodes=10, use_evaluation=True):
    """
    ç»Ÿä¸€çš„è¯„ä¼°å‡½æ•°ï¼Œæ”¯æŒä¸åŒç±»å‹çš„ç¯å¢ƒå’Œç½‘ç»œ
    
    Args:
        network: è®­ç»ƒå¥½çš„ç½‘ç»œï¼ˆéœ€è¦æœ‰get_actionæ–¹æ³•ï¼‰
        env: ç¯å¢ƒå®ä¾‹ï¼ˆGazeboEnvæˆ–VelodyneGymWrapperï¼‰
        epoch: å½“å‰è®­ç»ƒè½®æ¬¡
        eval_episodes: è¯„ä¼°å›åˆæ•°
        use_evaluation: æ˜¯å¦ä½¿ç”¨è¯„ä¼°æŒ‡æ ‡
    """
    if use_evaluation:
        # æ ¹æ®ç¯å¢ƒç±»å‹é€‰æ‹©åˆé€‚çš„è¯„ä¼°åŒ…è£…å™¨
        if isinstance(env, GazeboEnv):
            eval_env = GazeboEvaluationWrapper(env)
        elif hasattr(env, 'gazebo_env'):  # VelodyneGymWrapper
            eval_env = EvaluationWrapper(env.launchfile, env.environment_dim, env.action_type)
        else:
            eval_env = None
            print("Warning: Unknown environment type, evaluation metrics disabled")
    else:
        eval_env = None
    
    avg_reward = 0.0
    col = 0
    
    for _ in range(eval_episodes):
        count = 0
        state = env.reset()
        if eval_env:
            eval_env.reset()
        
        done = False
        while not done and count < 501:
            action = network.get_action(np.array(state))
            
            # å¤„ç†ä¸åŒç®—æ³•çš„åŠ¨ä½œæ ¼å¼
            if hasattr(network, 'num_actions'):  # DQNç®—æ³•
                a_in = [(action[0] + 1) / 2, action[1]]
            else:  # TD3ç­‰è¿ç»­åŠ¨ä½œç®—æ³•
                a_in = action
            
            state, reward, done, _ = env.step(a_in)
            if eval_env:
                eval_env.step(a_in)
            
            avg_reward += reward
            count += 1
            if reward < -90:
                col += 1
    
    avg_reward /= eval_episodes
    avg_col = col / eval_episodes
    
    print("..............................................")
    print(
        "Average Reward over %i Evaluation Episodes, Epoch %i: %f, %f"
        % (eval_episodes, epoch, avg_reward, avg_col)
    )
    print("..............................................")
    
    # æ‰“å°è¯„ä¼°æŒ‡æ ‡
    if eval_env:
        eval_env.print_evaluation_summary()
    
    return avg_reward


