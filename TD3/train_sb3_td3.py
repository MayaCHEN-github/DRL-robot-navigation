import os
import torch
import time
import os
import numpy as np
from stable_baselines3 import TD3
from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback

from gym_wrapper import VelodyneGymWrapper


class DetailedEvaluationCallback(BaseCallback):
    """è¯¦ç»†è¯„ä¼°å›è°ƒ: æ¯5000æ­¥è¿›è¡Œè¯¦ç»†è¯„ä¼°å¹¶ä¿å­˜ç»“æœ"""

    def __init__(self, env, eval_freq: int = 5000, eval_episodes: int = 10, verbose: int = 1):
        super().__init__(verbose)
        self.eval_freq = eval_freq
        self.eval_episodes = eval_episodes
        self.env = env  # ä¼ å…¥ç¯å¢ƒä»¥ä¾¿åœ¨å›è°ƒä¸­ä½¿ç”¨
        self.last_eval = 0

    def _on_step(self) -> bool:
        if self.num_timesteps - self.last_eval >= self.eval_freq:
            self.last_eval = self.num_timesteps
            if self.verbose > 0:
                print(f"\n=== è¯„ä¼°è§¦å‘ï¼ˆç¬¬{self.num_timesteps}æ­¥ï¼‰===")
                
            # è¿›è¡Œè¯¦ç»†è¯„ä¼°
            summary = self._evaluate_detailed(self.env, self.model, self.eval_episodes)
            
            # ä¿å­˜è¯„ä¼°ç»“æœåˆ°npyæ–‡ä»¶
            self._save_evaluation_results(summary, self.num_timesteps)
            
            # è®°å½•åˆ°TensorBoard
            self._log_evaluation_to_tensorboard(summary, self.num_timesteps)
        return True
    
    def _evaluate_detailed(self, env, model, eval_episodes):
        """æ‰§è¡Œè¯¦ç»†è¯„ä¼°å¹¶è®¡ç®—å„é¡¹æŒ‡æ ‡"""
        print(f"å¼€å§‹è¯¦ç»†è¯„ä¼°ï¼Œå…± {eval_episodes} ä¸ªå›åˆ...")
        
        # åˆå§‹åŒ–æŒ‡æ ‡
        episode_metrics = []
        success_episodes = 0
        collision_episodes = 0
        total_time_cost = 0.0
        total_path_efficiency = 0.0
        total_trajectory_smoothness = 0.0
        total_reward = 0.0
        
        for episode in range(eval_episodes):
            print(f"è¯„ä¼°å›åˆ {episode+1}/{eval_episodes}")
            
            # é‡ç½®ç¯å¢ƒ
            reset_result = env.reset()
            state = reset_result[0] if isinstance(reset_result, tuple) else reset_result
            if isinstance(state, tuple):
                state = state[0]
            state = np.array(state, dtype=np.float32)
            
            # åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡
            start_time = time.time()
            trajectory = []
            total_distance = 0.0
            episode_collision = False
            episode_reward = 0.0
            
            # è·å–èµ·å§‹ä½ç½®å’Œç›®æ ‡ä½ç½®
            if hasattr(env, 'gazebo_env'):
                start_x = env.gazebo_env.odom_x
                start_y = env.gazebo_env.odom_y
                goal_x = env.gazebo_env.goal_x
                goal_y = env.gazebo_env.goal_y
                trajectory.append((start_x, start_y))
                
                straight_line_distance = np.linalg.norm([goal_x - start_x, goal_y - start_y])
            else:
                # å¦‚æœæ— æ³•è·å–ä½ç½®ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤å€¼
                start_x = start_y = goal_x = goal_y = 0.0
                trajectory.append((start_x, start_y))
                straight_line_distance = 1.0
            
            done = False
            episode_steps = 0
            
            while not done and episode_steps < 500:  # è®¾ç½®æœ€å¤§æ­¥æ•°
                # ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
                action = model.predict(state, deterministic=True)[0]
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, terminated, truncated, info = env.step(action)
                done = terminated or truncated
                
                # æ›´æ–°çŠ¶æ€
                if isinstance(next_state, tuple):
                    next_state = next_state[0]
                state = np.array(next_state, dtype=np.float32)
                
                # è®°å½•æœ¬å›åˆå¥–åŠ±
                episode_reward += reward
                
                # è®°å½•è½¨è¿¹
                if hasattr(env, 'gazebo_env'):
                    current_x = env.gazebo_env.odom_x
                    current_y = env.gazebo_env.odom_y
                    trajectory.append((current_x, current_y))
                    
                    # è®¡ç®—æ­¥é•¿
                    if len(trajectory) > 1:
                        prev_x, prev_y = trajectory[-2]
                        step_distance = np.linalg.norm([current_x - prev_x, current_y - prev_y])
                        total_distance += step_distance
                
                # æ£€æŸ¥ç¢°æ’
                if reward < -90 and not episode_collision:
                    episode_collision = True
                
                episode_steps += 1
            
            # ç´¯è®¡æ€»å¥–åŠ±
            total_reward += episode_reward
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            time_cost = time.time() - start_time
            total_time_cost += time_cost
            
            # è®¡ç®—æˆåŠŸç‡
            if hasattr(env, 'gazebo_env'):
                final_x = env.gazebo_env.odom_x
                final_y = env.gazebo_env.odom_y
                final_distance = np.linalg.norm([goal_x - final_x, goal_y - final_y])
                success = 1.0 if final_distance < 0.3 else 0.0
            else:
                success = 0.0
            
            if success >= 0.5:
                success_episodes += 1
            
            if episode_collision:
                collision_episodes += 1
            
            # è®¡ç®—è·¯å¾„æ•ˆç‡ï¼ˆåªæœ‰æˆåŠŸåˆ°è¾¾ç›®æ ‡æ—¶æ‰è®¡ç®—ï¼Œå¤±è´¥æ—¶è®¾ä¸º0ï¼‰
            if success >= 0.5:  # æˆåŠŸåˆ°è¾¾ç›®æ ‡
                if total_distance > 0.1:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç§»åŠ¨è·ç¦»
                    path_efficiency = min(1.0, straight_line_distance / total_distance)
                else:
                    path_efficiency = 1.0  # æˆåŠŸä½†ç§»åŠ¨è·ç¦»å¾ˆå°ï¼Œè®¤ä¸ºæ•ˆç‡å¾ˆé«˜
            else:  # æœªæˆåŠŸåˆ°è¾¾ç›®æ ‡
                path_efficiency = 0.0  # å¤±è´¥æ—¶è·¯å¾„æ•ˆç‡ä¸º0
            
            total_path_efficiency += path_efficiency
            
            # è®¡ç®—è½¨è¿¹å¹³æ»‘åº¦
            smoothness = self._calculate_trajectory_smoothness(trajectory)
            total_trajectory_smoothness += smoothness
            
            # ä¿å­˜å›åˆæŒ‡æ ‡
            episode_metrics.append({
                'success_rate': success,
                'time_cost': time_cost,
                'path_efficiency': path_efficiency,
                'trajectory_smoothness': smoothness,
                'reward': episode_reward,
                # è°ƒè¯•æ•°æ®
                'debug_straight_line_distance': straight_line_distance,
                'debug_total_distance': total_distance,
                'debug_actual_distance': np.linalg.norm([final_x - start_x, final_y - start_y]) if hasattr(env, 'gazebo_env') else 0.0,
                'debug_start_pos': (start_x, start_y),
                'debug_final_pos': (final_x, final_y),
                'debug_goal_pos': (goal_x, goal_y),
                'debug_trajectory_length': len(trajectory),
                'debug_episode_steps': episode_steps
            })
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        n = len(episode_metrics) if episode_metrics else 1
        summary = {
            'episodes': n,
            'success_rate': success_episodes / n,
            'collision_rate': collision_episodes / n,
            'avg_time_cost': total_time_cost / n,
            'avg_path_efficiency': total_path_efficiency / n,
            'avg_trajectory_smoothness': total_trajectory_smoothness / n,
            'avg_reward': total_reward / n,
            # è°ƒè¯•æ•°æ®
            'avg_straight_line_distance': np.mean([m['debug_straight_line_distance'] for m in episode_metrics]),
            'avg_total_distance': np.mean([m['debug_total_distance'] for m in episode_metrics]),
            'avg_actual_distance': np.mean([m['debug_actual_distance'] for m in episode_metrics]),
            'avg_trajectory_length': np.mean([m['debug_trajectory_length'] for m in episode_metrics]),
            'avg_episode_steps': np.mean([m['debug_episode_steps'] for m in episode_metrics]),
        }
        
        # æ‰“å°è¯¦ç»†è¯„ä¼°ç»“æœ
        print(f"è¯¦ç»†è¯„ä¼°å®Œæˆ:")
        print(f"  æˆåŠŸç‡: {summary['success_rate']:.2%}")
        print(f"  ç¢°æ’ç‡: {summary['collision_rate']:.2%}")
        print(f"  å¹³å‡å¥–åŠ±: {summary['avg_reward']:.2f}")
        print(f"  å¹³å‡æ—¶é—´æˆæœ¬: {summary['avg_time_cost']:.2f}ç§’")
        print(f"  å¹³å‡è·¯å¾„æ•ˆç‡: {summary['avg_path_efficiency']:.2%}")
        print(f"  å¹³å‡è½¨è¿¹å¹³æ»‘åº¦: {summary['avg_trajectory_smoothness']:.2%}")
        print(f"ğŸ“Š è°ƒè¯•æ•°æ®:")
        print(f"  å¹³å‡ç›´çº¿è·ç¦»: {summary['avg_straight_line_distance']:.2f}m")
        print(f"  å¹³å‡æ€»è·ç¦»: {summary['avg_total_distance']:.2f}m")
        print(f"  å¹³å‡å®é™…è·ç¦»: {summary['avg_actual_distance']:.2f}m")
        print(f"  å¹³å‡è½¨è¿¹ç‚¹æ•°: {summary['avg_trajectory_length']:.1f}")
        print(f"  å¹³å‡å›åˆæ­¥æ•°: {summary['avg_episode_steps']:.1f}")
        print(f"ğŸ’¡ è¯´æ˜:")
        print(f"  - è·¯å¾„æ•ˆç‡: æˆåŠŸå›åˆçš„è·¯å¾„æ•ˆç‡ï¼Œå¤±è´¥å›åˆä¸º0%")
        print(f"  - æ•ˆç‡æ¯”å€¼: {summary['avg_straight_line_distance']:.2f} / {summary['avg_total_distance']:.2f} = {summary['avg_straight_line_distance']/summary['avg_total_distance']:.3f}")
        
        return summary
    
    def _calculate_trajectory_smoothness(self, trajectory):
        """è®¡ç®—è½¨è¿¹å¹³æ»‘åº¦ï¼ˆåŸºäºè½¨è¿¹ç‚¹çš„æ›²ç‡å˜åŒ–ï¼‰"""
        if len(trajectory) < 3:
            return 1.0  # è½¨è¿¹ç‚¹å¤ªå°‘ï¼Œè®¤ä¸ºå¹³æ»‘
        
        # è®¡ç®—è½¨è¿¹çš„æ›²ç‡å˜åŒ–
        curvature_changes = []
        for i in range(1, len(trajectory) - 1):
            p1 = np.array(trajectory[i-1])
            p2 = np.array(trajectory[i])
            p3 = np.array(trajectory[i+1])
            
            # è®¡ç®—ä¸¤ä¸ªå‘é‡çš„å¤¹è§’
            v1 = p2 - p1
            v2 = p3 - p2
            
            if np.linalg.norm(v1) > 0 and np.linalg.norm(v2) > 0:
                cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
                cos_angle = np.clip(cos_angle, -1, 1)  # é˜²æ­¢æ•°å€¼è¯¯å·®
                angle = np.arccos(cos_angle)
                curvature_changes.append(angle)
        
        if not curvature_changes:
            return 1.0
        
        # å¹³æ»‘åº¦ = 1 / (1 + å¹³å‡æ›²ç‡å˜åŒ–)
        avg_curvature = np.mean(curvature_changes)
        smoothness = 1.0 / (1.0 + avg_curvature)
        return smoothness
    
    def _save_evaluation_results(self, summary, timestep):
        """ä¿å­˜è¯„ä¼°ç»“æœåˆ°npyæ–‡ä»¶"""
        os.makedirs("./logs", exist_ok=True)
        detailed_evaluation_data = {
            'summary': summary,
            'timestamp': time.time(),
            'timestep': timestep
        }
        np.save("./logs/evaluation_metrics.npy", detailed_evaluation_data)
        
    def _log_evaluation_to_tensorboard(self, summary, timestep):
        """è®°å½•è¯„ä¼°ç»“æœåˆ°TensorBoard"""
        try:
            # è®°å½•è¯„ä¼°æŒ‡æ ‡åˆ°TensorBoard
            self.model.logger.record("evaluation/success_rate", summary['success_rate'])
            self.model.logger.record("evaluation/collision_rate", summary['collision_rate'])
            self.model.logger.record("evaluation/avg_time_cost", summary['avg_time_cost'])
            self.model.logger.record("evaluation/avg_path_efficiency", summary['avg_path_efficiency'])
            self.model.logger.record("evaluation/avg_trajectory_smoothness", summary['avg_trajectory_smoothness'])
            
            # æ–°å¢ï¼šè®°å½•å¹³å‡å¥–åŠ±
            if 'avg_reward' in summary:
                self.model.logger.record("evaluation/avg_reward", summary['avg_reward'])
            
            # è®°å½•è°ƒè¯•æ•°æ®
            self.model.logger.record("evaluation_debug/avg_straight_line_distance", summary['avg_straight_line_distance'])
            self.model.logger.record("evaluation_debug/avg_total_distance", summary['avg_total_distance'])
            self.model.logger.record("evaluation_debug/avg_actual_distance", summary['avg_actual_distance'])
            self.model.logger.record("evaluation_debug/avg_trajectory_length", summary['avg_trajectory_length'])
            self.model.logger.record("evaluation_debug/avg_episode_steps", summary['avg_episode_steps'])
            
            # å¼ºåˆ¶å†™å…¥TensorBoard
            self.model.logger.dump(step=timestep)
            
            print(f"è¯„ä¼°ç»“æœå·²è®°å½•åˆ°TensorBoardå’Œnpyæ–‡ä»¶")
            
        except Exception as e:
            print(f"è®°å½•è¯„ä¼°ç»“æœåˆ°TensorBoardæ—¶å‡ºé”™: {e}")


def main():
    # å¼ºåˆ¶ä½¿ç”¨CPU
    print("=== è®¾å¤‡è®¾ç½® ===")
    device = "cpu"
    print(f"å·²è®¾ç½®ä½¿ç”¨CPUè¿›è¡Œè®¡ç®—")
    print("=" * 20)

    # ç¯å¢ƒ
    print("æ­£åœ¨åˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
    env = VelodyneGymWrapper(
        launchfile="multi_robot_scenario.launch",
        environment_dim=20,
        action_type="continuous",
        device=device,
    )

    # TD3 æ¨¡å‹ï¼ˆåå‘åŠ¨ä½œå¹³æ»‘ä¸ç²¾ç»†æ§åˆ¶ï¼‰
    print("æ­£åœ¨åˆ›å»ºTD3æ¨¡å‹...")
    eval_freq = 5_000  # ä¸HRLä¸€è‡´ï¼Œè®¾ç½®ä¸º5000æ­¥
    # æ·»åŠ æ¢ç´¢å™ªå£°è¡°å‡å‚æ•°
    expl_noise = 1.0  # åˆå§‹æ¢ç´¢å™ªå£°
    expl_decay_steps = 500_000  # å™ªå£°è¡°å‡æ­¥æ•°
    expl_min = 0.1  # æœ€å°æ¢ç´¢å™ªå£°

    model = TD3(
        "MlpPolicy",
        env,
        verbose=0,
        tensorboard_log="./logs/td3_velodyne",
        learning_rate=1e-4,  # é™ä½å­¦ä¹ ç‡
        buffer_size=1_000_000,  # å¢åŠ ç¼“å†²åŒºå¤§å°
        batch_size=40,  # ä¸velodyneç‰ˆæœ¬ä¸€è‡´
        tau=0.005,
        gamma=0.99999,  # ä¸velodyneç‰ˆæœ¬ä¸€è‡´
        train_freq=1,
        gradient_steps=1,
        policy_delay=2,  # ä¸velodyneç‰ˆæœ¬ä¸€è‡´
        target_policy_noise=0.2,
        target_noise_clip=0.5,
        device=device,
    )

    print(f"âœ… TD3æ¨¡å‹å·²åˆ›å»ºåœ¨è®¾å¤‡: {device}")
    print(f"ğŸ“Š è¯„ä¼°è®¾ç½®:")
    print(f"  - è¯„ä¼°é¢‘ç‡: æ¯{eval_freq}æ­¥")

    # æ¨¡å‹ä¿å­˜ç›®å½•
    os.makedirs("./pytorch_models", exist_ok=True)

    checkpoint_callback = CheckpointCallback(
        save_freq=eval_freq,
        save_path="./pytorch_models/",
        name_prefix="td3_velodyne",
    )
    eval_callback = DetailedEvaluationCallback(env=env, eval_freq=eval_freq, eval_episodes=10, verbose=1)

    # å¿«é€ŸéªŒè¯
    print("=== ç¬¬ä¸€é˜¶æ®µï¼šå¿«é€ŸéªŒè¯ï¼ˆ5000æ­¥ï¼‰===")
    model.learn(total_timesteps=5_000, progress_bar=True, callback=[checkpoint_callback, eval_callback])
    print("âœ… ç¬¬ä¸€é˜¶æ®µå®Œæˆï¼ç¨‹åºè¿è¡Œæ­£å¸¸ã€‚")

    # æ­£å¼è®­ç»ƒï¼ˆå¯æŒ‰éœ€ç»§ç»­ï¼‰
    user_input = input("ç¨‹åºè¿è¡Œæ­£å¸¸ï¼æ˜¯å¦ç»§ç»­è®­ç»ƒæ›´å¤šæ­¥æ•°ï¼Ÿ(y/n): ")
    if user_input.lower() == "y":
        print("=== ç¬¬äºŒé˜¶æ®µï¼šæ­£å¼è®­ç»ƒï¼ˆ1,995,000æ­¥ï¼Œå»¶ç»­åŒä¸€æ¨¡å‹ä¸æ—¶é—´è½´ï¼‰===")
        model.learn(total_timesteps=1_995_000, progress_bar=True, callback=[checkpoint_callback, eval_callback], reset_num_timesteps=False)
        print("âœ… è®­ç»ƒå®Œæˆï¼")
    else:
        print("è®­ç»ƒå·²åœæ­¢ã€‚")

    print("æ­£åœ¨ä¿å­˜æ¨¡å‹...")
    model.save("./pytorch_models/td3_test_model")
    env.close()
    print("è®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    main()


